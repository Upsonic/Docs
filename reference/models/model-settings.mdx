---
title: ModelSettings
sidebarTitle: ModelSettings
---

## Parameters

| Parameter                          | Type                                                  | Default    | Description                                                                                                                                                                                                                      |
| ---------------------------------- | ----------------------------------------------------- | ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `max_tokens`                       | `int`                                                 | `None`     | The maximum number of tokens to generate before stopping. Supported by: Gemini, Anthropic, OpenAI, Groq, Cohere, Mistral, Bedrock, MCP Sampling                                                                                                                                                                                                      |
| `temperature`                      | `float`                                               | `None`     | Amount of randomness injected into the response. Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's maximum `temperature` for creative and generative tasks. Note that even with `temperature` of `0.0`, the results will not be fully deterministic. Supported by: Gemini, Anthropic, OpenAI, Groq, Cohere, Mistral, Bedrock                                                                                                                                                                                                      |
| `top_p`                            | `float`                                               | `None`     | An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. You should either alter `temperature` or `top_p`, but not both. Supported by: Gemini, Anthropic, OpenAI, Groq, Cohere, Mistral, Bedrock                                                                                                                                                                                                      |
| `timeout`                          | `float \| Timeout`                                    | `None`     | Override the client-level default timeout for a request, in seconds. Supported by: Gemini, Anthropic, OpenAI, Groq, Mistral                                                                                                                                                                                                      |
| `parallel_tool_calls`              | `bool`                                                | `None`     | Whether to allow parallel tool calls. Supported by: OpenAI (some models, not o1), Groq, Anthropic                                                                                                                                                                                                      |
| `seed`                             | `int`                                                 | `None`     | The random seed to use for the model, theoretically allowing for deterministic results. Supported by: OpenAI, Groq, Cohere, Mistral, Gemini                                                                                                                                                                                                      |
| `presence_penalty`                  | `float`                                               | `None`     | Penalize new tokens based on whether they have appeared in the text so far. Supported by: OpenAI, Groq, Cohere, Gemini, Mistral                                                                                                                                                                                                      |
| `frequency_penalty`                | `float`                                               | `None`     | Penalize new tokens based on their existing frequency in the text so far. Supported by: OpenAI, Groq, Cohere, Gemini, Mistral                                                                                                                                                                                                      |
| `logit_bias`                       | `dict[str, int]`                                      | `None`     | Modify the likelihood of specified tokens appearing in the completion. Supported by: OpenAI, Groq                                                                                                                                                                                                      |
| `stop_sequences`                   | `list[str]`                                           | `None`     | Sequences that will cause the model to stop generating. Supported by: OpenAI, Anthropic, Bedrock, Mistral, Groq, Cohere, Google                                                                                                                                                                                                      |
| `extra_headers`                     | `dict[str, str]`                                      | `None`     | Extra headers to send to the model. Supported by: OpenAI, Anthropic, Groq                                                                                                                                                                                                      |
| `extra_body`                       | `object`                                              | `None`     | Extra body to send to the model. Supported by: OpenAI, Anthropic, Groq                                                                                                                                                                                                      |

## Functions

### `merge_model_settings`

Merge two sets of model settings, preferring the overrides.

**Parameters:**

- `base` (ModelSettings | None): Base model settings
- `overrides` (ModelSettings | None): Override model settings

**Returns:**

- `ModelSettings | None`: Merged model settings, preferring the overrides

**Description:**

A common use case is: merge_model_settings(<agent settings>, <run settings>)
