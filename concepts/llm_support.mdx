# Model Creation Guide

This guide covers how to create model instances with custom settings and profiles in the Upsonic AI Agent Framework.

## Supported Models

The framework supports the latest models from major providers:

### OpenAI Models
- `gpt-5`, `gpt-5-mini`, `gpt-5-nano`
- `gpt-4o`, `gpt-4o-mini`
- `o1-preview`, `o1-mini`, `o1-pro` 
- `o3`, `o3-mini`, `o3-pro` 

### Anthropic Models
- `claude-3-5-sonnet-20241022`, `claude-3-5-haiku-20241022`
- `claude-3-7-sonnet-20250219`
- `claude-4-opus-20250514`, `claude-4-sonnet-20250514`
- `claude-opus-4-0`, `claude-sonnet-4-0`

### Google Models
- `gemini-2.5-flash`, `gemini-2.5-flash-lite`, `gemini-2.5-pro`
- `gemini-2.0-flash`, `gemini-2.0-flash-lite`

### Other Major Providers
- **Groq**: `llama-3.3-70b-versatile`, `llama-3.1-8b-instant`
- **Cohere**: `command-r-plus`, `command-r`, `c4ai-aya-expanse-32b`
- **Mistral**: `mistral-large-latest`, `codestral-latest`
- **Grok**: `grok-4`, `grok-3`, `grok-3-mini`
- **Bedrock**: Various Amazon, Anthropic, Meta, and Mistral models

## Environment Variables

Each provider requires specific environment variables for authentication:

### OpenAI
```bash
OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=https://api.openai.com/v1  # Optional
```

### Anthropic
```bash
ANTHROPIC_API_KEY=your_anthropic_api_key
```

### Google
```bash
# For Gemini API
GOOGLE_API_KEY=your_google_api_key
# OR (legacy)
GEMINI_API_KEY=your_gemini_api_key

# For Vertex AI
GOOGLE_CLOUD_PROJECT=your_project_id
GOOGLE_CLOUD_LOCATION=us-central1  # Optional
```

### Groq
```bash
GROQ_API_KEY=your_groq_api_key
GROQ_BASE_URL=https://api.groq.com  # Optional
```

### Cohere
```bash
CO_API_KEY=your_cohere_api_key
CO_BASE_URL=https://api.cohere.ai  # Optional
```

### Mistral
```bash
MISTRAL_API_KEY=your_mistral_api_key
```

### Hugging Face
```bash
HF_TOKEN=your_huggingface_token
```

### Grok
```bash
GROK_API_KEY=your_grok_api_key
```

### Azure OpenAI
```bash
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_KEY=your_azure_api_key
OPENAI_API_VERSION=2024-02-15-preview
```

### Other Providers
- **DeepSeek**: `DEEPSEEK_API_KEY`
- **Fireworks**: `FIREWORKS_API_KEY`
- **OpenRouter**: `OPENROUTER_API_KEY`
- **GitHub**: `GITHUB_API_KEY`
- **Ollama**: `OLLAMA_BASE_URL` (API key optional)

## Overview

Upsonic framework provides a flexible model system where you can create model instances with custom settings and profiles. All model classes inherit from a base `Model` class and support provider-specific settings that extend the base `ModelSettings` class.

## Base Model Class

All model classes inherit from the base `Model` class:

```python
class Model(ABC):
    def __init__(
        self,
        *,
        settings: ModelSettings | None = None,
        profile: ModelProfileSpec | None = None,
    ) -> None:
        """Initialize the model with optional settings and profile.

        Args:
            settings: Model-specific settings that will be used as defaults for this model.
            profile: The model profile to use.
        """
```

## Base ModelSettings

The base `ModelSettings` class provides common parameters supported across multiple providers:

```python
class ModelSettings(TypedDict, total=False):
    max_tokens: int
    """The maximum number of tokens to generate before stopping."""
    
    temperature: float
    """Amount of randomness injected into the response."""
    
    top_p: float
    """An alternative to sampling with temperature, called nucleus sampling."""
    
    timeout: float | Timeout
    """Override the client-level default timeout for a request, in seconds."""
    
    parallel_tool_calls: bool
    """Whether to allow parallel tool calls."""
    
    seed: int
    """The random seed to use for the model, theoretically allowing for deterministic results."""
    
    presence_penalty: float
    """Penalize new tokens based on whether they have appeared in the text so far."""
    
    frequency_penalty: float
    """Penalize new tokens based on their existing frequency in the text so far."""
    
    logit_bias: dict[str, int]
    """Modify the likelihood of specified tokens appearing in the completion."""
    
    stop_sequences: list[str]
    """Sequences that will cause the model to stop generating."""
    
    extra_headers: dict[str, str]
    """Extra headers to send to the model."""
    
    extra_body: object
    """Extra body to send to the model."""
```

## Provider-Specific Settings

Each provider extends the base `ModelSettings` with provider-specific parameters. All provider-specific parameters are prefixed with the provider name to avoid conflicts.

### Anthropic Settings

```python
class AnthropicModelSettings(ModelSettings, total=False):
    anthropic_metadata: BetaMetadataParam
    """An object describing metadata about the request.
    
    Contains `user_id`, an external identifier for the user who is associated with the request.
    """
    
    anthropic_thinking: BetaThinkingConfigParam
    """Determine whether the model should generate a thinking block.
    
    See [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for more information.
    """
```

### OpenAI Settings

```python
class OpenAIChatModelSettings(ModelSettings, total=False):
    openai_reasoning_effort: ReasoningEffort
    """Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).
    
    Currently supported values are `low`, `medium`, and `high`.
    """
    
    openai_logprobs: bool
    """Include log probabilities in the response."""
    
    openai_top_logprobs: int
    """Include log probabilities of the top n tokens in the response."""
    
    openai_user: str
    """A unique identifier representing the end-user, which can help OpenAI monitor and detect abuse."""
    
    openai_service_tier: Literal['auto', 'default', 'flex', 'priority']
    """The service tier to use for the model request."""
    
    openai_prediction: ChatCompletionPredictionContentParam
    """Enables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs)."""
```

### Google Settings

```python
class GoogleModelSettings(ModelSettings, total=False):
    google_safety_settings: list[SafetySettingDict]
    """The safety settings to use for the model.
    
    See <https://ai.google.dev/gemini-api/docs/safety-settings> for more information.
    """
    
    google_thinking_config: ThinkingConfigDict
    """The thinking configuration to use for the model.
    
    See <https://ai.google.dev/gemini-api/docs/thinking> for more information.
    """
    
    google_labels: dict[str, str]
    """User-defined metadata to break down billed charges. Only supported by the Vertex AI API."""
    
    google_video_resolution: MediaResolution
    """The video resolution to use for the model."""
    
    google_cached_content: str
    """The name of the cached content to use for the model."""
```

### Groq Settings

```python
class GroqModelSettings(ModelSettings, total=False):
    groq_reasoning_format: Literal['hidden', 'raw', 'parsed']
    """Controls how reasoning output is formatted."""
```

### Bedrock Settings

```python
class BedrockModelSettings(ModelSettings, total=False):
    bedrock_guardrail_config: GuardrailConfigurationTypeDef
    """Content moderation and safety settings for Bedrock API requests."""
    
    bedrock_performance_configuration: PerformanceConfigurationTypeDef
    """Performance optimization settings for model inference."""
    
    bedrock_request_metadata: dict[str, str]
    """Additional metadata to attach to Bedrock API requests."""
    
    bedrock_additional_model_response_fields_paths: list[str]
    """JSON paths to extract additional fields from model responses."""
    
    bedrock_prompt_variables: Mapping[str, PromptVariableValuesTypeDef]
    """Variables for substitution into prompt templates."""
    
    bedrock_additional_model_requests_fields: Mapping[str, Any]
    """Additional model-specific parameters to include in requests."""
```

## Model Creation Patterns

### Pattern 1: Create Model with Settings and Profile

```python
from upsonic.models.anthropic import AnthropicModel, AnthropicModelSettings
from upsonic.profiles import ModelProfile

# Create custom settings
settings = AnthropicModelSettings(
    max_tokens=2048,
    temperature=0.7,
    anthropic_thinking={
        "type": "enabled",
        "budget_tokens": 4096
    }
)

# Create custom profile
profile = ModelProfile(
    supports_tools=True,
    supports_json_schema_output=True,
    default_structured_output_mode='tool'
)

# Create model instance
model = AnthropicModel(
    model_name="claude-3-5-sonnet-20241022",
    settings=settings,
    profile=profile
)

# Create agent with the model
from upsonic import Agent
agent = Agent(model=model)
```

### Pattern 2: Create Agent with Model, Settings, and Profile

```python
from upsonic import Agent
from upsonic.models.openai import OpenAIChatModelSettings
from upsonic.profiles import ModelProfile

# Create settings
settings = OpenAIChatModelSettings(
    max_tokens=1024,
    temperature=0.5,
    openai_reasoning_effort="high",
    openai_reasoning_summary="detailed"
)

# Create profile
profile = ModelProfile(
    supports_tools=True,
    supports_json_object_output=True
)

# Create agent with model, settings, and profile
agent = Agent(
    model="openai/gpt-4o",
    settings=settings,
    profile=profile
)
```

### Pattern 3: Create Model with Provider-Specific Settings Only

```python
from upsonic.models.google import GoogleModel, GoogleModelSettings

# Create Google-specific settings
settings = GoogleModelSettings(
    max_tokens=1500,
    temperature=0.3,
    google_thinking_config={
        "include_thoughts": True,
        "thinking_budget": 2048
    },
    google_safety_settings=[
        {
            "category": "HARM_CATEGORY_HARASSMENT",
            "threshold": "BLOCK_MEDIUM_AND_ABOVE"
        }
    ]
)

# Create model
model = GoogleModel(
    model_name="gemini-2.5-flash-lite",
    settings=settings
)

# Create agent
from upsonic import Agent
agent = Agent(model=model)
```

## Usage Examples

### Anthropic with Thinking Enabled

```python
from upsonic import Agent, Task
from upsonic.models.anthropic import AnthropicModel, AnthropicModelSettings

# Configure thinking for complex problem solving
settings = AnthropicModelSettings(
    max_tokens=4096,
    temperature=0.1,
    anthropic_thinking={
        "type": "enabled",
        "budget_tokens": 8192
    }
)

model = AnthropicModel(
    model_name="claude-3-5-sonnet-20241022",
    settings=settings
)

agent = Agent(
    model=model,
    enable_thinking_tool=True  # Enable framework-level thinking
)

task = Task("Solve this complex mathematical proof step by step")
result = agent.do(task)
```

### OpenAI with High-Effort Reasoning

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel, OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    max_tokens=2048,
    temperature=0.2,
    openai_reasoning_effort="high",
    openai_reasoning_summary="detailed"
)

model = OpenAIChatModel(
    model_name="o1-preview",
    settings=settings
)

agent = Agent(model=model)

task = Task("Analyze this complex business strategy and provide detailed reasoning")
result = agent.do(task)
```

### Google with Thinking Configuration

```python
from upsonic import Agent, Task
from upsonic.models.google import GoogleModel, GoogleModelSettings

settings = GoogleModelSettings(
    max_tokens=1500,
    temperature=0.4,
    google_thinking_config={
        "include_thoughts": True,
        "thinking_budget": -1  # Automatic budget
    }
)

model = GoogleModel(
    model_name="gemini-2.5-flash-lite",
    settings=settings
)

agent = Agent(model=model)

task = Task("Explain quantum computing concepts with detailed reasoning")
result = agent.do(task)
```

### Groq with Reasoning Format

```python
from upsonic import Agent, Task
from upsonic.models.groq import GroqModel, GroqModelSettings

settings = GroqModelSettings(
    max_tokens=1024,
    temperature=0.6,
    groq_reasoning_format="parsed"  # Show parsed reasoning
)

model = GroqModel(
    model_name="llama-3.3-70b-versatile",
    settings=settings
)

agent = Agent(model=model)

task = Task("Write a creative story with reasoning")
result = agent.do(task)
```

## Best Practices

1. **Use Provider-Specific Settings**: Always use the appropriate provider-specific settings class for the model you're using.

2. **Combine Base and Provider Settings**: You can combine base `ModelSettings` parameters with provider-specific ones.

3. **Set Appropriate Budgets**: For thinking/reasoning features, set appropriate token budgets based on task complexity.

4. **Enable Framework Features**: Use `enable_thinking_tool=True` on the Agent for orchestrated thinking capabilities.

5. **Test Different Configurations**: Experiment with different settings to find the optimal configuration for your use case.

## Notes

- All provider-specific parameters are prefixed with the provider name (e.g., `anthropic_`, `openai_`, `google_`)
- Settings are merged when passed to both the model and agent
- Profile settings control how the model handles tools and structured output
- Some features like thinking/reasoning are only available on specific model variants
- Always check the provider documentation for the latest supported models and features
