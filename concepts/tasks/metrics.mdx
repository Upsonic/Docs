---
title: Task Metrics & Cost Tracking
sidebarTitle: Metrics
description: Track token usage and costs for your AI agent operations
---

## What are Task Metrics?

Task metrics provide detailed insights into the cost and token usage of your AI operations. Every task execution tracks input/output tokens, calculates costs based on the model used, and provides a unique identifier for tracking purposes.

## Why Track Metrics?

Tracking metrics enables you to:

- **Monitor costs**: Keep track of LLM API expenses in real-time
- **Optimize spending**: Identify expensive operations and optimize them
- **Budget management**: Set alerts and limits based on usage patterns
- **Performance analysis**: Understand token consumption patterns across tasks
- **Billing & reporting**: Generate cost reports for stakeholders

## Available Metrics

After executing a task, you can access the following metrics:

| Attribute | Type | Description |
|-----------|------|-------------|
| `total_cost` | float | Total cost in USD for the task execution |
| `total_input_token` | int | Number of input tokens processed |
| `total_output_token` | int | Number of output tokens generated |
| `price_id` | str | Unique identifier for the pricing model used |
| `get_total_cost()` | method | Method to retrieve the total cost |

## Basic Usage

### Tracking Agent Costs

```python
from upsonic import Agent, Task

# Create agent and task
agent = Agent(model="openai/gpt-4o")
task = Task("Summarize the key benefits of AI agents in production systems")

# Execute task
result = agent.do(task)

# Access metrics
print(f"Cost: ${task.total_cost:.6f}")
print(f"Input tokens: {task.total_input_token}")
print(f"Output tokens: {task.total_output_token}")
print(f"Price ID: {task.price_id}")
```

**Output:**
```
Cost: $0.000011
Input tokens: 68
Output tokens: 45
Price ID: 939ab57e-6a77-45e2-b593-f65212056bcd
```

### Tracking Direct LLM Call Costs

```python
from upsonic import Task
from upsonic.client import LLMClient

# Create LLM client
client = LLMClient(model="openai/gpt-4o")

# Create task
task = Task("What are the benefits of RAG systems?")

# Execute with client
response = client.chat([{"role": "user", "content": task.description}])

# Metrics are automatically tracked in the task
print(f"Total cost: ${task.get_total_cost():.6f}")
print(f"Tokens used: {task.total_input_token + task.total_output_token}")
```

## Advanced Usage

### Cost Monitoring Across Multiple Tasks

```python
from upsonic import Agent, Task

agent = Agent(model="openai/gpt-4o")

tasks = [
    Task("Analyze sentiment of customer feedback"),
    Task("Extract key entities from the document"),
    Task("Generate a summary report")
]

total_cost = 0
total_tokens = 0

for task in tasks:
    result = agent.do(task)
    
    cost = task.total_cost
    tokens = task.total_input_token + task.total_output_token
    
    total_cost += cost
    total_tokens += tokens
    
    print(f"Task: {task.description[:40]}...")
    print(f"  Cost: ${cost:.6f}")
    print(f"  Tokens: {tokens}\n")

print(f"Total cost: ${total_cost:.6f}")
print(f"Total tokens: {total_tokens}")
```

### Budget Alerting

```python
from upsonic import Agent, Task

def execute_with_budget(agent, task, max_cost=0.01):
    """Execute task with cost limit"""
    result = agent.do(task)
    
    if task.total_cost > max_cost:
        print(f"âš ï¸  Warning: Task exceeded budget!")
        print(f"   Expected: ${max_cost}")
        print(f"   Actual: ${task.total_cost:.6f}")
    
    return result

agent = Agent(model="openai/gpt-4o")
task = Task("Generate a detailed analysis report")

result = execute_with_budget(agent, task, max_cost=0.001)
```

### Cost Tracking Decorator

```python
from upsonic import Agent, Task
from functools import wraps

def track_costs(func):
    """Decorator to track and log task costs"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        result = func(*args, **kwargs)
        
        # Assuming the function returns a task
        if hasattr(result, 'total_cost'):
            print(f"ðŸ’° Cost: ${result.total_cost:.6f}")
            print(f"ðŸ“Š Tokens: {result.total_input_token + result.total_output_token}")
        
        return result
    return wrapper

@track_costs
def analyze_data(data):
    agent = Agent(model="openai/gpt-4o")
    task = Task(f"Analyze this data: {data}")
    agent.do(task)
    return task

# Usage
task = analyze_data("Sample customer feedback data")
```

## Cost Comparison

Compare costs across different models:

```python
from upsonic import Agent, Task

models = [
    "openai/gpt-4o",
    "openai/gpt-4o-mini",
    "anthropic/claude-sonnet-4-5"
]

task_description = "Explain quantum computing in simple terms"

print("Cost Comparison Across Models:\n")

for model in models:
    agent = Agent(model=model)
    task = Task(task_description)
    
    result = agent.do(task)
    
    print(f"{model}")
    print(f"  Cost: ${task.total_cost:.6f}")
    print(f"  Tokens: {task.total_input_token + task.total_output_token}")
    print()
```

## Best Practices

### 1. Monitor Regularly

```python
# Create a cost tracking context manager
class CostTracker:
    def __init__(self):
        self.total_cost = 0
        self.task_count = 0
    
    def track(self, task):
        self.total_cost += task.total_cost
        self.task_count += 1
    
    def summary(self):
        avg_cost = self.total_cost / self.task_count if self.task_count > 0 else 0
        return {
            "total_cost": self.total_cost,
            "task_count": self.task_count,
            "average_cost": avg_cost
        }

tracker = CostTracker()
agent = Agent(model="openai/gpt-4o")

for i in range(10):
    task = Task(f"Process item {i}")
    agent.do(task)
    tracker.track(task)

stats = tracker.summary()
print(f"Processed {stats['task_count']} tasks")
print(f"Total cost: ${stats['total_cost']:.6f}")
print(f"Average cost per task: ${stats['average_cost']:.6f}")
```

### 2. Set Cost Limits

```python
class CostLimitExceeded(Exception):
    pass

def execute_with_limit(agent, task, cost_limit=0.05):
    """Execute task with hard cost limit"""
    result = agent.do(task)
    
    if task.total_cost > cost_limit:
        raise CostLimitExceeded(
            f"Task cost ${task.total_cost:.6f} exceeded limit ${cost_limit}"
        )
    
    return result
```

### 3. Log for Analysis

```python
import json
from datetime import datetime

def log_task_metrics(task, filename="task_metrics.jsonl"):
    """Log task metrics for later analysis"""
    metrics = {
        "timestamp": datetime.now().isoformat(),
        "description": task.description[:100],
        "cost": task.total_cost,
        "input_tokens": task.total_input_token,
        "output_tokens": task.total_output_token,
        "price_id": task.price_id
    }
    
    with open(filename, "a") as f:
        f.write(json.dumps(metrics) + "\n")

# Usage
agent = Agent(model="openai/gpt-4o")
task = Task("Analyze customer sentiment")
agent.do(task)
log_task_metrics(task)
```

## Understanding Price IDs

Each model and pricing tier has a unique `price_id`. This identifier:

- Links to the specific pricing model used
- Remains consistent for the same model/tier combination
- Helps track pricing changes over time
- Can be used for detailed billing breakdowns

```python
from upsonic import Agent, Task

agent = Agent(model="openai/gpt-4o")
task = Task("Sample query")
agent.do(task)

print(f"Model: {agent.model}")
print(f"Price ID: {task.price_id}")
print(f"This ID is used for: {agent.model} pricing calculations")
```

## Cost Optimization Tips

1. **Choose the right model**: Use smaller models (gpt-4o-mini) for simpler tasks
2. **Optimize prompts**: Shorter, clearer prompts use fewer input tokens
3. **Cache results**: Use [task caching](/concepts/tasks/task-caching) for repeated queries
4. **Batch operations**: Process multiple items in a single task when possible
5. **Monitor trends**: Track metrics over time to identify optimization opportunities

## Related Documentation

- [Task Caching](/concepts/tasks/task-caching) - Reduce costs with caching
- [Task Attributes](/concepts/tasks/attributes) - All available task attributes
- [Task Results](/concepts/tasks/results) - Accessing task execution results
- [Agent Overview](/concepts/agents/overview) - Learn about agents

## Troubleshooting

### Metrics Not Available

If metrics are not populated, ensure:
- The task has been executed
- The model provider supports token counting
- The task completed successfully

```python
task = Task("Test query")
result = agent.do(task)

if task.total_cost is None:
    print("Metrics not available - check task execution")
else:
    print(f"Metrics available: ${task.total_cost:.6f}")
```

### Unexpected Costs

If costs are higher than expected:
- Check the model being used (gpt-4o vs gpt-4o-mini)
- Review input token count (includes system prompts, tools, context)
- Verify output token count (longer responses cost more)
- Consider using [response format](/concepts/tasks/response-format) to limit output

<Note>
  **Cost Tracking Accuracy**
  
  Costs are calculated based on the model provider's pricing at execution time. Token counts are precise, and costs are calculated using the latest pricing information from each provider.
</Note>


