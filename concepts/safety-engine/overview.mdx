---
title: "Safety Engine"
description: "Content safety and policy enforcement for AI agents"
sidebarTitle: Overview
hideToc: true
---

## What is the Safety Engine?

The Safety Engine is Upsonic's guardrail system that enforces policies on agent behavior. It acts as a filter that controls what information flows into your agents (user input) and what comes out (agent responses). Think of it as a security checkpoint that validates and transforms content based on your rules.

The Safety Engine uses a policy-based approach where you define rules (what to detect) and actions (what to do when detected). Policies can block content, anonymize sensitive data, replace text, or raise exceptions.

## Why use the Safety Engine?

The Safety Engine is essential for production AI systems:

- **Compliance**: Meet regulatory requirements (GDPR, HIPAA, etc.) by detecting and handling PII
- **Brand safety**: Prevent agents from generating inappropriate or harmful content
- **Risk mitigation**: Block or anonymize sensitive information before it reaches LLMs or users
- **Policy enforcement**: Ensure agents follow company policies and guidelines
- **Tool validation**: Verify tools are safe before registration and execution
- **Multi-language support**: Automatically adapt safety checks to user's language
- **Custom rules**: Create organization-specific policies for your use case

**This is Upsonic's most differentiating feature in the market.** While other frameworks focus on capabilities, we prioritize safetyâ€”because production AI needs guardrails.

## Overview

Safety Engine validates content at multiple checkpoints: user input, agent responses, and tool execution. It includes pre-built policies for common safety concerns (PII, hate speech, adult content, etc.) and supports custom policies for organization-specific needs.

## Key Features

- **Input/Output Filtering**: Validate user input and agent responses
- **Tool Safety Policies**: Validate tools at registration and before execution
- **Pre-built Policies**: Ready-to-use policies for PII, adult content, hate speech, profanity, etc.
- **Custom Policies**: Create your own rules and actions
- **Multiple Actions**: Block, anonymize, replace, or raise exceptions
- **Multi-language Support**: Automatically adapts to user's language
- **LLM-Powered Detection**: Use LLMs for context-aware content detection

## Example

```python
from upsonic import Agent, Task
from upsonic.safety_engine.policies.pii_policies import PIIAnonymizePolicy

# Create agent with PII anonymization
agent = Agent(
    "openai/gpt-5",
    agent_policy=PIIAnonymizePolicy
)

# User input with PII
task = Task(
    description="My email is john.doe@example.com and phone is 555-1234. What are my email and phone?"
)

# Execute - PII will be anonymized in output
result = agent.do(task)
print(result)  # PII like email and phone will be anonymized
```

## Tool Safety Policies

Tool safety policies provide two validation points:

- **Pre-execution (`tool_policy_pre`)**: Validates tools during registration before task execution
- **Post-execution (`tool_policy_post`)**: Validates tool calls before execution when LLM invokes a tool

```python
from upsonic import Agent, Task
from upsonic.tools import tool
from upsonic.safety_engine.policies.tool_safety_policies import HarmfulToolBlockPolicy

# Apply tool safety policies
agent = Agent(
    "openai/gpt-5",
    name="Test Agent",
    tool_policy_pre=HarmfulToolBlockPolicy,   # Validate at registration
    tool_policy_post=HarmfulToolBlockPolicy,   # Validate before execution
    debug=True  # Enable debug to see policy logs
)

# Create a potentially harmful tool to test tool_policy_pre (registration validation)
@tool
def delete_file(filepath: str) -> str:
    """Delete a file from the system."""
    import os
    if os.path.exists(filepath):
        os.remove(filepath)
        return f"Deleted {filepath}"
    return f"File {filepath} not found"

# Create a task object that will test both policies:
# - tool_policy_pre: Validates the delete_file tool when it's registered
# - tool_policy_post: Validates the tool call before execution
task = Task(
    description="Use the delete_file tool to delete /tmp/test_file.txt",
    tools=[delete_file]
)

# Execute the task - this will trigger both tool_policy_pre and tool_policy_post if it passes tool_policy_pre!
result = agent.do(task)
print(f"Task result: {result}")
```

## Navigation

- [Pre-built Policies](/concepts/safety-engine/prebuilt-policies/pii-policies) - Ready-to-use policies for PII, adult content, hate speech, profanity, and more
- [Custom Policy](/concepts/safety-engine/custom-policy/creating-policy) - Create your own safety policies with custom rules and actions
- [Creating Rules](/concepts/safety-engine/custom-policy/creating-rule) - Define custom detection rules for content filtering
- [Creating Actions](/concepts/safety-engine/custom-policy/creating-action) - Configure actions for policy violations