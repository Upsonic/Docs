---
title: "Model Gateways"
description: "Access multiple LLM providers through unified gateways"
---

## Overview

Model gateways provide unified access to multiple LLM providers through a single API. They handle provider routing, fallbacks, rate limiting, and cost optimization. Upsonic supports OpenRouter, LiteLLM, and other popular gateways.

---

## OpenRouter

OpenRouter provides access to 100+ models from different providers through a unified OpenAI-compatible API.

### Authentication

Get your API key from [openrouter.ai](https://openrouter.ai):

```bash
export OPENROUTER_API_KEY="sk-or-v1-..."
```

### Available Models

OpenRouter provides access to models from:
- OpenAI (GPT-4, GPT-3.5, o1)
- Anthropic (Claude 3.5, Claude 4)
- Google (Gemini 2.0, Gemini 2.5)
- Meta (Llama 3, Llama 3.1, Llama 3.2, Llama 3.3)
- Mistral (Mistral Large, Mixtral)
- Cohere (Command R+)
- Many more open-source models

See all available models at [openrouter.ai/models](https://openrouter.ai/models)

### Basic Example

```python
from upsonic import Agent, Task

# Use OpenRouter with any supported model
agent = Agent(model="openrouter/anthropic/claude-3.5-sonnet")
task = Task("Explain neural networks")
result = agent.do(task)
```

### With Multiple Providers

```python
from upsonic import Agent, Task

# Try different providers through OpenRouter
models = [
    "openrouter/anthropic/claude-3.5-sonnet",
    "openrouter/google/gemini-2.5-flash",
    "openrouter/meta-llama/llama-3.3-70b-instruct",
    "openrouter/mistralai/mistral-large"
]

for model_name in models:
    agent = Agent(model=model_name)
    result = agent.do(Task("What is 2+2?"))
    print(f"{model_name}: {result}")
```

### Advanced Configuration

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel, OpenAIChatModelSettings
from upsonic.providers.openai import openai_provider

# Create OpenRouter provider
provider = openai_provider(
    provider_name="openrouter",
    base_url="https://openrouter.ai/api/v1",
    api_key="your-api-key"
)

# Configure settings
settings = OpenAIChatModelSettings(
    max_tokens=2048,
    temperature=0.7,
    extra_headers={
        "HTTP-Referer": "https://your-app.com",  # Required for OpenRouter
        "X-Title": "My AI App"  # Optional
    }
)

model = OpenAIChatModel(
    model_name="anthropic/claude-3.5-sonnet",
    provider=provider,
    settings=settings
)

agent = Agent(model=model)
```

### Model Routing

OpenRouter supports automatic fallbacks:

```python
from upsonic.models.openai import OpenAIChatModelSettings

# Use "auto" to let OpenRouter choose the best available model
settings = OpenAIChatModelSettings(
    extra_body={
        "models": [
            "anthropic/claude-3.5-sonnet",
            "google/gemini-2.5-flash",
            "meta-llama/llama-3.3-70b-instruct"
        ],
        "route": "fallback"  # Try models in order
    }
)
```

### Cost Optimization

OpenRouter shows costs per request:

```python
# Check model pricing at openrouter.ai/models
# Prices vary by model

# Use cheaper models for simple tasks
agent_cheap = Agent(model="openrouter/google/gemini-2.5-flash")

# Use expensive models for complex tasks
agent_premium = Agent(model="openrouter/anthropic/claude-4-opus")
```

### Streaming Example

```python
from upsonic import Agent, Task

agent = Agent(model="openrouter/anthropic/claude-3.5-sonnet")
task = Task("Write a detailed story")

async for chunk in agent.run_stream(task):
    print(chunk, end="", flush=True)
```

### Prompt Caching

OpenRouter supports prompt caching for supported models (like Claude):

```python
from upsonic import Agent

# Claude models on OpenRouter automatically use caching
agent = Agent(
    model="openrouter/anthropic/claude-3.5-sonnet",
    system_prompt="[Long system prompt...]"  # Will be cached
)
```

### OpenRouter-Specific Features

| Feature | Description |
|---------|-------------|
| **Model Fallbacks** | Automatic fallback to alternative models |
| **Provider Selection** | Choose from 20+ providers |
| **Rate Limiting** | Built-in rate limit management |
| **Cost Tracking** | Detailed usage and cost reporting |
| **Credits System** | Prepaid credits for easier billing |
| **Free Models** | Some models available for free |

### OpenRouter Parameters

```python
from upsonic.models.openai import OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    extra_body={
        "transforms": ["middle-out"],  # Response transformations
        "models": ["model1", "model2"],  # Fallback models
        "route": "fallback",  # Routing strategy
        "provider": {
            "order": ["Anthropic", "Google"],  # Provider preference
            "allow_fallbacks": True
        }
    },
    extra_headers={
        "HTTP-Referer": "https://your-app.com",  # Required
        "X-Title": "Your App Name"  # Optional but recommended
    }
)
```

---

## LiteLLM

LiteLLM is a unified interface to 100+ LLMs with a self-hosted proxy.

### Installation

**Via Docker:**
```bash
docker run -p 4000:4000 ghcr.io/berriai/litellm:main-latest
```

**Via Python:**
```bash
pip install litellm[proxy]
litellm --port 4000
```

### Authentication

LiteLLM proxies your provider credentials:

```bash
# Configure providers in config.yaml
model_list:
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: os.environ/OPENAI_API_KEY
  
  - model_name: claude-3
    litellm_params:
      model: anthropic/claude-3-5-sonnet
      api_key: os.environ/ANTHROPIC_API_KEY
```

### Basic Example

```python
from upsonic import Agent, Task
import os

# Point to LiteLLM proxy
os.environ["OPENAI_BASE_URL"] = "http://localhost:4000"
os.environ["OPENAI_API_KEY"] = "sk-1234"  # Your LiteLLM key

# Use any model through LiteLLM
agent = Agent(model="litellm/gpt-4")
task = Task("Explain quantum computing")
result = agent.do(task)
```

### Multiple Providers

```python
# Access different providers through single endpoint
agent_openai = Agent(model="litellm/gpt-4")
agent_anthropic = Agent(model="litellm/claude-3")
agent_gemini = Agent(model="litellm/gemini-2.5-flash")
```

### Load Balancing

LiteLLM supports load balancing across multiple keys:

```yaml
# config.yaml
model_list:
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: key1
  
  - model_name: gpt-4
    litellm_params:
      model: openai/gpt-4
      api_key: key2
```

### Cost Tracking

LiteLLM provides detailed cost tracking and budgets:

```yaml
# config.yaml
general_settings:
  master_key: sk-1234
  database_url: postgresql://...  # For cost tracking
  
litellm_settings:
  success_callback: ["langfuse"]  # Track to Langfuse
```

### Caching

LiteLLM supports semantic caching:

```yaml
# config.yaml
litellm_settings:
  cache: True
  cache_params:
    type: redis
    host: localhost
    port: 6379
```

### LiteLLM Parameters

```python
from upsonic.models.openai import OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    extra_headers={
        "Authorization": "Bearer sk-1234"  # LiteLLM key
    },
    extra_body={
        "caching": True,  # Enable caching
        "mock_response": "test"  # For testing
    }
)
```

---

## Together AI

Together AI provides access to open-source models with fast inference.

### Authentication

```bash
export TOGETHER_API_KEY="..."
```

### Available Models

Together AI hosts open-source models:
- Meta Llama (3, 3.1, 3.2, 3.3)
- Mistral and Mixtral
- Qwen
- DeepSeek
- And many more

### Basic Example

```python
from upsonic import Agent, Task

# Together AI uses OpenAI-compatible API
agent = Agent(model="together/meta-llama/Llama-3.3-70B-Instruct-Turbo")
task = Task("Write Python code")
result = agent.do(task)
```

### Advanced Configuration

```python
from upsonic.models.openai import OpenAIChatModel, OpenAIChatModelSettings
from upsonic.providers.openai import openai_provider

provider = openai_provider(
    provider_name="together",
    base_url="https://api.together.xyz/v1",
    api_key="your-key"
)

settings = OpenAIChatModelSettings(
    max_tokens=2048,
    temperature=0.7
)

model = OpenAIChatModel(
    model_name="meta-llama/Llama-3.3-70B-Instruct-Turbo",
    provider=provider,
    settings=settings
)

agent = Agent(model=model)
```

### Prompt Caching

Together AI does not currently support prompt caching.

---

## Vercel AI

Vercel AI provides serverless access to multiple providers.

### Authentication

```bash
export VERCEL_API_KEY="..."
```

### Basic Example

```python
from upsonic import Agent, Task

agent = Agent(model="vercel/anthropic/claude-3-5-sonnet")
task = Task("Analyze this data")
result = agent.do(task)
```

---

## HuggingFace Inference API

Access 10,000+ models through HuggingFace.

### Authentication

```bash
export HF_TOKEN="hf_..."
```

### Basic Example

```python
from upsonic import Agent, Task
from upsonic.models.huggingface import HuggingFaceModel

model = HuggingFaceModel(
    model_name="meta-llama/Llama-3.3-70B-Instruct",
    provider="huggingface"
)

agent = Agent(model=model)
task = Task("Explain AI")
result = agent.do(task)
```

### Available Models

Browse models at [huggingface.co/models](https://huggingface.co/models):
- Meta Llama models
- Mistral models
- Qwen models
- Phi models
- Many research models

---

## Comparison: Model Gateways

| Gateway | Providers | Key Feature | Best For |
|---------|-----------|-------------|----------|
| **OpenRouter** | 20+ | Easy access, free tier | Quick start, cost optimization |
| **LiteLLM** | 100+ | Self-hosted, caching | Enterprise, control |
| **Together AI** | Open-source | Fast inference | Open models, speed |
| **Vercel AI** | Multiple | Serverless | Edge deployments |
| **HuggingFace** | 10,000+ | Research models | Experimentation |

---

## Choosing a Gateway

### Use OpenRouter When:
- You want easy access to multiple providers
- You need automatic fallbacks
- You want to compare models easily
- Cost tracking is important
- You want a free tier to start

### Use LiteLLM When:
- You need self-hosted solution
- You want semantic caching
- Load balancing is required
- You need detailed cost tracking
- Enterprise security is critical

### Use Together AI When:
- You only need open-source models
- Fast inference is priority
- You want competitive pricing
- You need good performance

### Use Vercel AI When:
- You're building on Vercel
- You need edge deployment
- Serverless architecture is preferred

### Use HuggingFace When:
- You're experimenting with models
- You need access to research models
- You want to try many models
- Community models are important

---

## Best Practices

### 1. Use Fallbacks

```python
settings = OpenAIChatModelSettings(
    extra_body={
        "models": [
            "anthropic/claude-3.5-sonnet",  # Primary
            "google/gemini-2.5-flash",       # Fallback
            "meta-llama/llama-3.3-70b"       # Final fallback
        ],
        "route": "fallback"
    }
)
```

### 2. Monitor Costs

```python
# Track usage across providers
# Use gateway's dashboard for cost monitoring
```

### 3. Handle Rate Limits

```python
from upsonic.utils.package.exception import ModelHTTPError
import asyncio

async def call_with_retry(agent, task, max_retries=3):
    for attempt in range(max_retries):
        try:
            return await agent.async_do(task)
        except ModelHTTPError as e:
            if e.status_code == 429:  # Rate limit
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
            else:
                raise
```

### 4. Use Appropriate Models

```python
# Simple tasks - use cheap models
agent_simple = Agent(model="openrouter/google/gemini-2.5-flash")

# Complex tasks - use premium models
agent_complex = Agent(model="openrouter/anthropic/claude-4-opus")
```

### 5. Set HTTP Referer (OpenRouter)

```python
settings = OpenAIChatModelSettings(
    extra_headers={
        "HTTP-Referer": "https://your-app.com",  # Required
        "X-Title": "Your App"  # Recommended
    }
)
```

---

## Security Considerations

### API Key Protection

```python
import os

# Never hardcode API keys
api_key = os.getenv("OPENROUTER_API_KEY")

# Use key management services in production
# - AWS Secrets Manager
# - Azure Key Vault
# - Google Secret Manager
```

### Rate Limiting

Implement client-side rate limiting:

```python
import asyncio
from asyncio import Semaphore

semaphore = Semaphore(5)  # Max 5 concurrent requests

async def rate_limited_call(agent, task):
    async with semaphore:
        return await agent.async_do(task)
```

### Request Validation

```python
# Validate inputs before sending
def validate_input(text: str) -> str:
    if len(text) > 10000:
        raise ValueError("Input too long")
    return text
```

---

## Next Steps

- [Native Model Providers](/native-providers) - OpenAI, Anthropic, Google
- [Cloud Model Providers](/cloud-providers) - AWS Bedrock, Azure OpenAI
- [Local Model Providers](/local-providers) - Ollama
- [OpenAI-Compatible Models](/openai-compatible) - Using OpenAI-compatible endpoints

