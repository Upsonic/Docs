---
title: "Overview"
description: "Understanding LLM models and error handling in Upsonic"
---

Large Language Models (LLMs) are the foundation of the Upsonic AI Agent Framework. The framework provides a unified interface to interact with various LLM providers, allowing you to build AI agents that can leverage different models without changing your code structure.

In Upsonic, all model classes inherit from the base `Model` class, which provides:

- **Unified Interface**: Consistent API across all providers
- **UEL Integration**: Models implement the `Runnable` interface for chain composition
- **Streaming Support**: Real-time response streaming for better UX
- **Tool Calling**: Native function calling capabilities
- **Structured Output**: Type-safe responses using Pydantic models
- **Memory Management**: Built-in conversation history support

Model settings control the behavior of LLM requests. All settings are optional and provider-specific settings are prefixed with the provider name (e.g., `openai_`, `anthropic_`, `google_`).

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel, OpenAIChatModelSettings
from upsonic.models.anthropic import AnthropicModel
from upsonic.models.google import GoogleModel
from upsonic.uel import ChatPromptTemplate

# Direct instantiation
model = OpenAIChatModel(model_name="gpt-4o")
agent = Agent(model=model)

task = Task("Explain quantum computing")
result = agent.do(task)

# With custom settings
settings = OpenAIChatModelSettings(
    max_tokens=1024,
    temperature=0.5,
    openai_reasoning_effort="high"
)

model = OpenAIChatModel(
    model_name="gpt-4o",
    settings=settings
)

agent = Agent(model=model)

# UEL Chains
prompt = ChatPromptTemplate.from_template("Tell me about {topic}")
model = OpenAIChatModel(model_name="gpt-4o")

# Chain composition with pipe operator
chain = prompt | model
result = await chain.invoke({"topic": "AI"})
```

## Error Handling

The framework provides comprehensive error handling for all LLM operations:

```python
import asyncio
from pydantic import BaseModel, ValidationError
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel
from upsonic.models.settings import ModelSettings
from upsonic.models import ModelRequestParameters, override_allow_model_requests
from upsonic.utils.package.exception import ModelHTTPError, UserError, UnexpectedModelBehavior

# ModelHTTPError - HTTP errors during model requests
try:
    model = OpenAIChatModel(model_name="gpt-4o")
    agent = Agent(model=model)
    task = Task("Hello")
    result = agent.do(task)
except ModelHTTPError as e:
    print(f"Status Code: {e.status_code}")
    print(f"Model: {e.model_name}")
    print(f"Body: {e.body}")

# UserError - Configuration or usage errors
try:
    model = OpenAIChatModel(model_name="invalid-model-name")
except UserError as e:
    print(f"Error: {e}")

# UnexpectedModelBehavior - Unexpected model responses
try:
    model_settings = ModelSettings(max_tokens=2048)
    model_request_parameters = ModelRequestParameters()
    async with model.request_stream(messages, model_settings, model_request_parameters) as stream:
        pass
except UnexpectedModelBehavior as e:
    print(f"Unexpected behavior: {e}")

# Rate Limit Handling with Retry
async def request_with_retry(agent, task, max_retries=3):
    for attempt in range(max_retries):
        try:
            return agent.do(task)
        except ModelHTTPError as e:
            if e.status_code == 429:  # Rate limit
                wait_time = 2 ** attempt  # Exponential backoff
                print(f"Rate limited. Waiting {wait_time}s...")
                await asyncio.sleep(wait_time)
            else:
                raise
    raise Exception("Max retries exceeded")

# Token Limit Handling
try:
    settings = ModelSettings(max_tokens=4096)
    model = OpenAIChatModel(model_name="gpt-4o")
    agent = Agent(model=model, settings=settings)
    result = agent.do(very_long_task)
except ModelHTTPError as e:
    if "context_length_exceeded" in str(e.body):
        print("Input too long, consider truncating")

# Invalid Response Handling with Structured Output
class ResponseFormat(BaseModel):
    answer: str
    confidence: float

try:
    model = OpenAIChatModel(model_name="gpt-4o")
    model = model.with_structured_output(ResponseFormat)
    result = await model.ainvoke("What is AI?")
except ValidationError as e:
    print(f"Invalid response format: {e}")

# Global Error Handling - Disable requests for testing
with override_allow_model_requests(False):
    try:
        result = agent.do(task)
    except RuntimeError as e:
        print("Model requests are disabled")
```

