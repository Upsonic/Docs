---
title: "LLM"
description: "Understanding LLM models and error handling in Upsonic"
sidebarTitle: Overview
hideToc: true
---

## What is LLM Model?

Large Language Models (LLMs) are the foundation of the Upsonic AI Agent Framework. The framework provides a unified interface to interact with various LLM providers, allowing you to build AI agents that can leverage different models without changing your code structure.

In Upsonic, all model classes inherit from the base `Model` class, which provides:

- **Unified Interface**: Consistent API across all providers
- **UEL Integration**: Models implement the `Runnable` interface for chain composition
- **Streaming Support**: Real-time response streaming for better UX
- **Tool Calling**: Native function calling capabilities
- **Structured Output**: Type-safe responses using Pydantic models
- **Memory Management**: Built-in conversation history support

## Examples

### With Model Classes

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel

model = OpenAIChatModel(model_name="gpt-4o")
agent = Agent(model=model)

task = Task("Hello, how are you?")
result = agent.do(task)
print(result)
```

### With Model as String

```python
from upsonic import Agent, Task

agent = Agent(model="openai/gpt-4o")

task = Task("Hello, how are you?")
result = agent.do(task)
print(result)
```

## Supported LLM Models

Upsonic supports a wide range of LLM providers, from native APIs to local deployments and cloud-based solutions.

### Native Providers

Direct integration with major LLM provider APIs:
- [OpenAI Chat](/concepts/llm-support/native-providers/native-providers-openai-chat)
- [OpenAI Responses](/concepts/llm-support/native-providers/native-providers-openai-responses)
- [Anthropic](/concepts/llm-support/native-providers/native-providers-anthropic)
- [Google](/concepts/llm-support/native-providers/native-providers-google)
- [Mistral](/concepts/llm-support/native-providers/native-providers-mistral)
- [Cohere](/concepts/llm-support/native-providers/native-providers-cohere)
- [Grok](/concepts/llm-support/native-providers/native-providers-grok)

### Local Providers

Run models locally on your infrastructure:
- [Ollama](/concepts/llm-support/local-providers/local-providers-ollama)
- [vLLM](/concepts/llm-support/local-providers/local-providers-vllm)

### Cloud Providers

Enterprise cloud platform integrations:
- [Azure OpenAI](/concepts/llm-support/cloud-providers/cloud-providers-azure)
- [AWS Bedrock](/concepts/llm-support/cloud-providers/cloud-providers-bedrock)
- [Hugging Face](/concepts/llm-support/cloud-providers/cloud-providers-huggingface)

### Model Gateways

Unified access to multiple providers through gateway services:
- [NVIDIA NIM](/concepts/llm-support/model-gateways/model-gateways-nvidia)
- [Groq](/concepts/llm-support/model-gateways/model-gateways-groq)
- [LiteLLM](/concepts/llm-support/model-gateways/model-gateways-litellm)
- [OpenRouter](/concepts/llm-support/model-gateways/model-gateways-openrouter)
