---
title: "Local Model Providers"
description: "Run LLMs locally with Ollama and other providers"
---

## Overview

Local model providers allow you to run language models on your own hardware, providing complete data privacy, no API costs, and offline capabilities. Upsonic supports local deployment through Ollama and other OpenAI-compatible local servers.

---

## Ollama

Ollama is the easiest way to run large language models locally. It provides a simple interface to download, run, and manage models on your machine.

### Installation

**macOS/Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**macOS (Homebrew):**
```bash
brew install ollama
```

**Windows:**
Download from [ollama.com](https://ollama.com/download)

**Start Ollama server:**
```bash
ollama serve
```

### Authentication

Ollama runs locally and typically doesn't require authentication. Configure the base URL:

```bash
export OLLAMA_BASE_URL="http://localhost:11434"  # Default
```

### Available Models

Ollama supports many open-source models:

**Popular Models:**
- **Llama 3**: `llama3:8b`, `llama3:70b`, `llama3.1:8b`, `llama3.1:70b`, `llama3.1:405b`
- **Llama 3.2**: `llama3.2:1b`, `llama3.2:3b`
- **Llama 3.3**: `llama3.3:70b`
- **Mistral**: `mistral:7b`, `mistral:latest`
- **Mixtral**: `mixtral:8x7b`, `mixtral:8x22b`
- **Gemma**: `gemma2:2b`, `gemma2:9b`, `gemma2:27b`
- **Qwen**: `qwen2.5:7b`, `qwen2.5:14b`, `qwen2.5:32b`, `qwen2.5:72b`
- **DeepSeek**: `deepseek-r1:1.5b`, `deepseek-r1:7b`, `deepseek-r1:8b`, `deepseek-r1:14b`, `deepseek-r1:32b`, `deepseek-r1:70b`
- **Phi**: `phi3:mini`, `phi3:medium`
- **CodeLlama**: `codellama:7b`, `codellama:13b`, `codellama:34b`

See all models at [ollama.com/library](https://ollama.com/library)

### Pull a Model

Before using a model, download it:

```bash
# Download Llama 3.3 70B
ollama pull llama3.3:70b

# Download Mistral 7B
ollama pull mistral:7b

# Download DeepSeek R1
ollama pull deepseek-r1:8b
```

### Basic Example

```python
from upsonic import Agent, Task
from upsonic.providers.openai import openai_provider

# Create Ollama provider
provider = openai_provider(
    provider_name="ollama",
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # Not required but needed for initialization
)

# Create model
from upsonic.models.openai import OpenAIChatModel

model = OpenAIChatModel(
    model_name="llama3.3:70b",
    provider=provider
)

agent = Agent(model=model)
task = Task("Explain quantum computing")
result = agent.do(task)
print(result)
```

### Simpler Usage Pattern

```python
from upsonic import Agent, Task
import os

# Set Ollama endpoint
os.environ["OPENAI_BASE_URL"] = "http://localhost:11434/v1"
os.environ["OPENAI_API_KEY"] = "ollama"  # Dummy key

agent = Agent(model="openai/llama3.3:70b")
task = Task("Write a Python function")
result = agent.do(task)
```

### With Tools

```python
from upsonic import Agent, Task
from upsonic.tools import ToolConfig

def calculate(expression: str) -> str:
    """Calculate a mathematical expression."""
    return str(eval(expression))

agent = Agent(
    model="openai/llama3.3:70b",  # Via Ollama
    tools=[calculate]
)

task = Task("What is 25 * 37?")
result = agent.do(task)
```

### Streaming Example

```python
from upsonic import Agent, Task

agent = Agent(model="openai/llama3.3:70b")
task = Task("Write a short story")

async for chunk in agent.run_stream(task):
    print(chunk, end="", flush=True)
```

### Model Management

**List installed models:**
```bash
ollama list
```

**Remove a model:**
```bash
ollama rm llama3.3:70b
```

**Show model info:**
```bash
ollama show llama3.3:70b
```

**Run a model directly:**
```bash
ollama run llama3.3:70b "Explain AI"
```

### Custom Models

Create custom models with a Modelfile:

```dockerfile
# Modelfile
FROM llama3.3:70b

# Set temperature
PARAMETER temperature 0.7

# Set system prompt
SYSTEM """
You are a helpful AI assistant specializing in Python programming.
"""
```

**Create and use custom model:**
```bash
ollama create my-python-assistant -f Modelfile
```

```python
agent = Agent(model="openai/my-python-assistant")
```

### Performance Optimization

**1. GPU Acceleration**

Ollama automatically uses available GPUs. Check with:
```bash
ollama ps
```

**2. Model Quantization**

Use quantized models for faster inference:
- `llama3.3:70b-q4_0` - 4-bit quantization
- `llama3.3:70b-q8_0` - 8-bit quantization

```python
agent = Agent(model="openai/llama3.3:70b-q4_0")
```

**3. Context Window**

Configure context window size:
```bash
ollama run llama3.3:70b --context 8192
```

**4. Concurrent Requests**

Ollama can handle multiple requests:
```bash
OLLAMA_MAX_LOADED_MODELS=2 ollama serve
```

### Prompt Caching

Ollama does not support explicit prompt caching, but keeps models in memory for faster subsequent requests.

### Ollama-Specific Parameters

Since Ollama uses OpenAI-compatible API, use `OpenAIChatModelSettings`:

```python
from upsonic.models.openai import OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    max_tokens=2048,
    temperature=0.7,
    top_p=0.9,
    stop_sequences=["User:", "Assistant:"]
)
```

**Note:** Not all OpenAI parameters are supported. Ollama supports:
- `temperature`
- `top_p`
- `max_tokens` (via `num_predict` internally)
- `stop_sequences` (via `stop` internally)

### Hardware Requirements

| Model Size | RAM Required | GPU VRAM | Speed |
|------------|--------------|----------|-------|
| 1-3B | 4-8 GB | 4 GB | Very Fast |
| 7-8B | 8-16 GB | 8 GB | Fast |
| 13-14B | 16-32 GB | 16 GB | Medium |
| 30-34B | 32-64 GB | 24 GB | Slower |
| 70B | 64-128 GB | 48 GB | Slow |
| 405B | 256 GB+ | 80 GB x4 | Very Slow |

**Quantization helps:**
- Q4: ~4 bits per parameter (4x smaller)
- Q5: ~5 bits per parameter
- Q8: ~8 bits per parameter (2x smaller)

### Recommended Models by Use Case

**For Development (Fast):**
- `llama3.2:3b` - Very fast, good for testing
- `phi3:mini` - Efficient, good reasoning
- `gemma2:2b` - Fast, lightweight

**For Production (Balanced):**
- `llama3.3:70b-q4_0` - Best quality/speed trade-off
- `mistral:7b` - Good performance, efficient
- `qwen2.5:14b` - Excellent reasoning

**For Maximum Quality:**
- `llama3.3:70b` - Best open model
- `qwen2.5:72b` - Strong performance
- `mixtral:8x22b` - High quality

**For Code:**
- `codellama:13b` - Code-specific
- `qwen2.5-coder:7b` - Excellent at coding
- `deepseek-r1:8b` - Reasoning for code

**For Reasoning:**
- `deepseek-r1:70b` - Best reasoning
- `qwen2.5:72b` - Strong reasoning
- `llama3.3:70b` - Balanced reasoning

### Limitations

- ❌ No built-in web search or code execution tools
- ❌ No native JSON schema validation (basic tool calling only)
- ❌ Performance depends on local hardware
- ❌ Larger models require significant resources
- ⚠️ Some features may not work exactly like OpenAI API

### Advantages

- ✅ Complete data privacy - no data sent to external APIs
- ✅ No API costs - unlimited usage
- ✅ Offline capability - works without internet
- ✅ Low latency - no network round-trip
- ✅ Customizable - create custom models
- ✅ Open source models

---

## LM Studio

LM Studio is another popular option for running models locally with a GUI.

### Installation

Download from [lmstudio.ai](https://lmstudio.ai)

### Usage

```python
import os

# LM Studio runs on port 1234 by default
os.environ["OPENAI_BASE_URL"] = "http://localhost:1234/v1"
os.environ["OPENAI_API_KEY"] = "lm-studio"

from upsonic import Agent, Task

agent = Agent(model="openai/local-model")
task = Task("Explain neural networks")
result = agent.do(task)
```

---

## LocalAI

LocalAI is a drop-in replacement for OpenAI API with local execution.

### Installation

```bash
docker run -p 8080:8080 localai/localai:latest
```

### Usage

```python
import os

os.environ["OPENAI_BASE_URL"] = "http://localhost:8080/v1"
os.environ["OPENAI_API_KEY"] = "local"

from upsonic import Agent, Task

agent = Agent(model="openai/gpt-4")  # Uses local model
task = Task("Summarize this text")
result = agent.do(task)
```

---

## Jan

Jan is a desktop application for running models locally.

### Installation

Download from [jan.ai](https://jan.ai)

### Usage

Similar to Ollama and LM Studio, Jan provides an OpenAI-compatible API.

---

## Text Generation WebUI

Popular WebUI for running models with many features.

### Installation

```bash
git clone https://github.com/oobabooga/text-generation-webui
cd text-generation-webui
./start_linux.sh  # or start_windows.bat, start_macos.sh
```

### Usage

Enable the OpenAI API extension and configure:

```python
import os

os.environ["OPENAI_BASE_URL"] = "http://localhost:5000/v1"
os.environ["OPENAI_API_KEY"] = "dummy"

from upsonic import Agent, Task

agent = Agent(model="openai/local-model")
task = Task("Write code")
result = agent.do(task)
```

---

## Comparison: Local Providers

| Provider | GUI | Easy Setup | Model Management | Best For |
|----------|-----|------------|------------------|----------|
| **Ollama** | ❌ | ✅ Excellent | ✅ CLI | Command-line users, automation |
| **LM Studio** | ✅ | ✅ Excellent | ✅ GUI | Desktop users, beginners |
| **LocalAI** | ❌ | ⚠️ Docker | ⚠️ Complex | Docker environments |
| **Jan** | ✅ | ✅ Good | ✅ GUI | Desktop users |
| **Text Generation WebUI** | ✅ | ⚠️ Complex | ✅ GUI | Advanced users, fine-tuning |

---

## Best Practices

### 1. Start Small

Test with smaller models first:
```bash
ollama pull llama3.2:3b
```

### 2. Monitor Resources

```bash
# Check running models
ollama ps

# Monitor system resources
htop  # or top on macOS
nvidia-smi  # for GPU
```

### 3. Optimize for Your Hardware

- **8GB RAM**: Use 3B models with Q4 quantization
- **16GB RAM**: Use 7B models with Q4/Q5 quantization
- **32GB RAM**: Use 13B models or 7B without quantization
- **64GB+ RAM**: Use 30B+ models

### 4. Keep Models Updated

```bash
ollama pull llama3.3:70b  # Updates to latest version
```

### 5. Use Appropriate Models

- **Chat/General**: Llama 3.3, Mistral, Qwen
- **Code**: CodeLlama, Qwen Coder, DeepSeek
- **Reasoning**: DeepSeek R1, QwQ
- **Fast/Lightweight**: Phi, Gemma 2B

---

## Troubleshooting

### Model Not Found

```bash
# Pull the model first
ollama pull llama3.3:70b
```

### Out of Memory

- Use smaller model or quantized version
- Close other applications
- Reduce context window

### Slow Performance

- Use GPU if available
- Try quantized models (Q4, Q5)
- Use smaller models for development
- Increase system swap/page file

### Connection Refused

```bash
# Make sure Ollama is running
ollama serve

# Check if port is in use
lsof -i :11434  # macOS/Linux
netstat -an | findstr 11434  # Windows
```

---

## Next Steps

- [Native Model Providers](/native-providers) - OpenAI, Anthropic, Google
- [Cloud Model Providers](/cloud-providers) - AWS Bedrock, Azure OpenAI
- [Model Gateways](/model-gateways) - OpenRouter, LiteLLM
- [OpenAI-Compatible Models](/openai-compatible) - Using OpenAI-compatible endpoints

