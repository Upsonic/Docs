---
title: "LM Studio"
description: "Using LM Studio for local model deployment with Upsonic"
---

## Overview

LM Studio lets you run large language models locally with an OpenAI-compatible API. Itâ€™s useful for development, testing, and privacy-sensitive use cases.

**Model Class:** `OpenAIChatModel` (OpenAI-compatible API)

## Authentication

```bash
export LMSTUDIO_BASE_URL="http://localhost:1234/v1"  # Required
```

## Examples

```python
from upsonic import Agent, Task
from upsonic.models.lmstudio import LMStudioModel

model = LMStudioModel(model_name="local-model")
agent = Agent(model=model)

task = Task("Hello, how are you?")
result = agent.do(task)
print(result)
```

## Model Settings

You can set model parameters in two ways: on the model or on the Agent.

**On the model:**

```python
from upsonic import Agent, Task
from upsonic.models.lmstudio import LMStudioModel, LMStudioModelSettings

model = LMStudioModel(
    model_name="local-model",
    settings=LMStudioModelSettings(max_tokens=1024, temperature=0.7)
)
agent = Agent(model=model)
```

**On the Agent:**

```python
from upsonic import Agent, Task
from upsonic.models.lmstudio import LMStudioModelSettings

agent = Agent(
    model="lmstudio/local-model",
    settings=LMStudioModelSettings(max_tokens=1024, temperature=0.7)
)
```

## Parameters

| Parameter | Type | Description | Default | Source |
|-----------|------|-------------|---------|--------|
| `max_tokens` | `int` | Maximum tokens to generate | Model default | Base |
| `temperature` | `float` | Sampling temperature | 0.8 | Base |
| `top_p` | `float` | Nucleus sampling | 0.9 | Base |
| `seed` | `int` | Random seed | None | Base |
| `stop_sequences` | `list[str]` | Stop sequences | None | Base |
