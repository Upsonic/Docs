---
title: "OpenAI-Compatible Models"
description: "Using OpenAI-compatible API endpoints with Upsonic"
---

## What are OpenAI-Compatible Models?

OpenAI-compatible models implement the same API interface as OpenAI's Chat Completions API, making it easy to switch between providers without changing code. Many providers offer OpenAI-compatible endpoints to simplify integration.

### Key Benefits

- **Easy Migration**: Switch providers without code changes
- **Unified Interface**: Same API across multiple providers
- **Drop-in Replacement**: Works with existing OpenAI code
- **Wide Support**: Many providers offer compatibility
- **Simplified Integration**: Single SDK for multiple providers

### API Compatibility

OpenAI-compatible providers implement these endpoints:
- `/v1/chat/completions` - Chat completions
- `/v1/completions` - Text completions (legacy)
- `/v1/embeddings` - Text embeddings
- `/v1/models` - List available models

---

## Supported Providers

Upsonic supports these OpenAI-compatible providers through the `OpenAIChatModel` class:

| Provider | Full Name | Specialty | Notes |
|----------|-----------|-----------|-------|
| **deepseek** | DeepSeek | Reasoning models | R1 models with chain-of-thought |
| **cerebras** | Cerebras | Ultra-fast inference | Industry-leading speed |
| **fireworks** | Fireworks AI | Open models | Fast inference, multiple models |
| **github** | GitHub Models | Free tier | Good for testing and development |
| **grok** | xAI Grok | Latest from xAI | Grok 2, 3, 4 models |
| **heroku** | Heroku AI | Managed hosting | Integrated with Heroku platform |
| **moonshotai** | Moonshot AI | Chinese models | Kimi models |
| **openrouter** | OpenRouter | Gateway | Access to 100+ models |
| **together** | Together AI | Open-source | Community models |
| **vercel** | Vercel AI | Serverless | Edge deployments |
| **litellm** | LiteLLM | Self-hosted gateway | Proxy for 100+ providers |
| **azure** | Azure OpenAI | Enterprise | Microsoft cloud integration |

---

## Basic Usage

### Simple Pattern

The easiest way to use OpenAI-compatible providers:

```python
from upsonic import Agent, Task

# Just prefix the model name with the provider
agent = Agent(model="deepseek/deepseek-chat")
task = Task("Explain quantum computing")
result = agent.do(task)
```

### Supported Provider Prefixes

```python
# DeepSeek
agent = Agent(model="deepseek/deepseek-chat")
agent = Agent(model="deepseek/deepseek-reasoner")

# Cerebras
agent = Agent(model="cerebras/llama-3.3-70b")

# Fireworks
agent = Agent(model="fireworks/llama-v3-70b-instruct")

# GitHub Models
agent = Agent(model="github/gpt-4o")

# Grok
agent = Agent(model="grok/grok-4")

# Together AI
agent = Agent(model="together/meta-llama/Llama-3.3-70B-Instruct-Turbo")

# OpenRouter
agent = Agent(model="openrouter/anthropic/claude-3.5-sonnet")

# Azure OpenAI
agent = Agent(model="azure/gpt-4o")
```

---

## Advanced Configuration

### Using Custom Providers

Create a custom provider for any OpenAI-compatible endpoint:

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel
from upsonic.providers.openai import openai_provider

# Create custom provider
provider = openai_provider(
    provider_name="my-provider",
    base_url="https://api.my-provider.com/v1",
    api_key="your-api-key"
)

# Create model
model = OpenAIChatModel(
    model_name="custom-model",
    provider=provider
)

agent = Agent(model=model)
task = Task("Your task")
result = agent.do(task)
```

### With Environment Variables

```bash
export OPENAI_BASE_URL="https://api.custom-provider.com/v1"
export OPENAI_API_KEY="your-key"
```

```python
from upsonic import Agent, Task

# Will use the custom endpoint
agent = Agent(model="openai/any-model-name")
task = Task("Your task")
result = agent.do(task)
```

### With Settings

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel, OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    max_tokens=2048,
    temperature=0.7,
    top_p=0.9,
    seed=42,
    presence_penalty=0.1,
    frequency_penalty=0.1
)

model = OpenAIChatModel(
    model_name="custom-model",
    provider="custom-provider",
    settings=settings
)

agent = Agent(model=model)
```

---

## Provider-Specific Examples

### DeepSeek

DeepSeek provides reasoning models with R1:

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModelSettings

# R1 models show reasoning process
settings = OpenAIChatModelSettings(
    max_tokens=4096,
    temperature=0.1
)

agent = Agent(
    model="deepseek/deepseek-reasoner",
    settings=settings
)

task = Task("Solve this complex problem step by step")
result = agent.do(task)
```

**Environment Variables:**
```bash
export DEEPSEEK_API_KEY="your-key"
```

**Available Models:**
- `deepseek-chat` - General purpose
- `deepseek-reasoner` - R1 reasoning model

---

### Cerebras

Cerebras provides ultra-fast inference:

```python
from upsonic import Agent, Task

# Cerebras is optimized for speed
agent = Agent(model="cerebras/llama-3.3-70b")
task = Task("Quick response needed")
result = agent.do(task)
```

**Environment Variables:**
```bash
export CEREBRAS_API_KEY="your-key"
```

**Available Models:**
- `llama-3.3-70b` - Llama 3.3 70B
- `llama-4-scout-17b-16e-instruct` - Llama 4 Scout
- `llama-4-maverick-17b-128e-instruct` - Llama 4 Maverick
- `qwen-3-235b-a22b-instruct-2507` - Qwen 3
- `qwen-3-235b-a22b-thinking-2507` - Qwen 3 Thinking

---

### Fireworks AI

Fireworks provides fast inference for open models:

```python
from upsonic import Agent, Task

agent = Agent(model="fireworks/llama-v3-70b-instruct")
task = Task("Your task")
result = agent.do(task)
```

**Environment Variables:**
```bash
export FIREWORKS_API_KEY="your-key"
```

**Available Models:** Check [fireworks.ai/models](https://fireworks.ai/models)

---

### GitHub Models

GitHub provides free access to popular models:

```python
from upsonic import Agent, Task

# Free tier available
agent = Agent(model="github/gpt-4o")
task = Task("Test task")
result = agent.do(task)
```

**Environment Variables:**
```bash
export GITHUB_TOKEN="ghp_..."
```

**Available Models:**
- OpenAI models (GPT-4, GPT-3.5)
- Anthropic Claude
- Meta Llama
- Mistral

---

### Grok (xAI)

Access xAI's Grok models:

```python
from upsonic import Agent, Task

agent = Agent(model="grok/grok-4")
task = Task("Your task")
result = agent.do(task)
```

**Environment Variables:**
```bash
export GROK_API_KEY="xai-..."
```

**Available Models:**
- `grok-4` - Latest Grok model
- `grok-4-0709` - Specific version
- `grok-3` - Previous generation
- `grok-3-mini` - Smaller, faster
- `grok-3-fast` - Optimized for speed
- `grok-2-vision-1212` - Vision capabilities

---

### Heroku AI

Integrated AI for Heroku platform:

```python
from upsonic import Agent, Task

agent = Agent(model="heroku/claude-3-5-sonnet-latest")
task = Task("Your task")
result = agent.do(task)
```

**Available Models:**
- Claude models
- Nova models
- GPT models

---

### Moonshot AI (Kimi)

Chinese AI models from Moonshot:

```python
from upsonic import Agent, Task

agent = Agent(model="moonshotai/kimi-latest")
task = Task("Your task")
result = agent.do(task)
```

**Environment Variables:**
```bash
export MOONSHOT_API_KEY="your-key"
```

**Available Models:**
- `moonshot-v1-8k` - 8K context
- `moonshot-v1-32k` - 32K context
- `moonshot-v1-128k` - 128K context
- `kimi-latest` - Latest Kimi model
- `kimi-thinking-preview` - Reasoning model
- `kimi-k2-0711-preview` - K2 preview

---

## Feature Support

Not all OpenAI-compatible providers support all OpenAI features:

| Feature | Support Varies By Provider |
|---------|---------------------------|
| **Streaming** | ✅ Most support |
| **Function Calling** | ⚠️ Some support |
| **JSON Mode** | ⚠️ Limited support |
| **JSON Schema** | ⚠️ Very limited |
| **Vision** | ❌ Rarely supported |
| **Tools** | ⚠️ Basic support |

### Testing Compatibility

```python
from upsonic import Agent, Task
from upsonic.utils.package.exception import ModelHTTPError

try:
    # Test if streaming works
    agent = Agent(model="provider/model")
    task = Task("Test")
    
    async for chunk in agent.run_stream(task):
        print("Streaming works!")
        break
        
except ModelHTTPError as e:
    print(f"Streaming not supported: {e}")
```

---

## Common Parameters

All OpenAI-compatible providers support these base parameters:

```python
from upsonic.models.openai import OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    max_tokens=2048,          # Maximum tokens to generate
    temperature=0.7,          # Randomness (0.0-2.0)
    top_p=0.9,               # Nucleus sampling
    seed=42,                 # Random seed
    stop_sequences=["END"],  # Stop sequences
    presence_penalty=0.0,    # Penalize repeated topics
    frequency_penalty=0.0,   # Penalize repeated tokens
)
```

### Provider-Specific Parameters

Some providers support additional parameters via `extra_body`:

```python
settings = OpenAIChatModelSettings(
    extra_body={
        "top_k": 50,              # Top-K sampling (some providers)
        "repetition_penalty": 1.1, # Repetition penalty (some providers)
        "min_p": 0.1,             # Min-P sampling (some providers)
    }
)
```

---

## Authentication Methods

### Method 1: Environment Variables (Recommended)

```bash
export PROVIDER_API_KEY="your-key"
export PROVIDER_BASE_URL="https://api.provider.com/v1"
```

### Method 2: Direct Configuration

```python
from upsonic.providers.openai import openai_provider

provider = openai_provider(
    provider_name="custom",
    base_url="https://api.provider.com/v1",
    api_key="your-key"
)
```

### Method 3: Dynamic Configuration

```python
import os

# Set at runtime
os.environ["OPENAI_BASE_URL"] = "https://api.provider.com/v1"
os.environ["OPENAI_API_KEY"] = "your-key"

agent = Agent(model="openai/model-name")
```

---

## Switching Between Providers

### Easy Provider Switching

```python
from upsonic import Agent, Task

# Test the same task across providers
providers = [
    "openai/gpt-4o",
    "deepseek/deepseek-chat",
    "cerebras/llama-3.3-70b",
    "fireworks/llama-v3-70b-instruct"
]

task = Task("Explain machine learning")

for provider in providers:
    agent = Agent(model=provider)
    result = agent.do(task)
    print(f"\n{provider}:\n{result}\n")
```

### Fallback Strategy

```python
from upsonic import Agent, Task
from upsonic.utils.package.exception import ModelHTTPError

async def call_with_fallback(task: Task, providers: list[str]):
    for provider in providers:
        try:
            agent = Agent(model=provider)
            return await agent.async_do(task)
        except ModelHTTPError as e:
            print(f"{provider} failed: {e}")
            continue
    raise RuntimeError("All providers failed")

# Try providers in order
providers = [
    "openai/gpt-4o",          # Primary
    "deepseek/deepseek-chat",  # Fallback 1
    "cerebras/llama-3.3-70b"   # Fallback 2
]

result = await call_with_fallback(task, providers)
```

---

## Performance Comparison

Approximate inference speeds (may vary):

| Provider | Speed | Cost | Best For |
|----------|-------|------|----------|
| **Cerebras** | ⚡⚡⚡⚡⚡ | $$ | Real-time apps |
| **Groq** | ⚡⚡⚡⚡ | $ | Fast responses |
| **Fireworks** | ⚡⚡⚡ | $$ | Balanced |
| **Together** | ⚡⚡⚡ | $$ | Open models |
| **DeepSeek** | ⚡⚡ | $ | Reasoning |
| **OpenAI** | ⚡⚡ | $$$ | Quality |

---

## Troubleshooting

### Connection Refused

```python
# Check if base URL is correct
import os
print(os.getenv("OPENAI_BASE_URL"))

# Verify API key is set
print(os.getenv("PROVIDER_API_KEY"))
```

### Unsupported Features

```python
from upsonic.utils.package.exception import ModelHTTPError

try:
    result = agent.do(task)
except ModelHTTPError as e:
    if e.status_code == 400:
        print("Feature not supported by provider")
```

### Rate Limiting

```python
import asyncio
from upsonic.utils.package.exception import ModelHTTPError

async def with_retry(agent, task, max_retries=3):
    for i in range(max_retries):
        try:
            return await agent.async_do(task)
        except ModelHTTPError as e:
            if e.status_code == 429:  # Rate limit
                wait = 2 ** i  # Exponential backoff
                await asyncio.sleep(wait)
            else:
                raise
```

---

## Best Practices

### 1. Set Timeouts

```python
from upsonic.models.openai import OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    timeout=30.0  # 30 seconds
)
```

### 2. Handle Errors Gracefully

```python
from upsonic.utils.package.exception import ModelHTTPError, UserError

try:
    result = agent.do(task)
except ModelHTTPError as e:
    print(f"HTTP {e.status_code}: {e.body}")
except UserError as e:
    print(f"Configuration error: {e}")
```

### 3. Use Provider-Specific Settings

```python
# Check provider documentation for supported parameters
settings = OpenAIChatModelSettings(
    temperature=0.7,  # Standard
    extra_body={
        "top_k": 50  # Provider-specific
    }
)
```

### 4. Test Before Production

```python
# Test with a simple task first
test_task = Task("Say 'Hello'")
try:
    result = agent.do(test_task)
    print("Provider works!")
except Exception as e:
    print(f"Provider test failed: {e}")
```

### 5. Monitor Usage

```python
# Log API calls for monitoring
import logging

logging.basicConfig(level=logging.INFO)
result = agent.do(task)  # Will log API calls
```

---

## Migration Guide

### From OpenAI to Other Providers

```python
# Before (OpenAI)
agent = Agent(model="openai/gpt-4o")

# After (DeepSeek)
agent = Agent(model="deepseek/deepseek-chat")

# After (Cerebras)
agent = Agent(model="cerebras/llama-3.3-70b")

# Code remains the same!
task = Task("Your task")
result = agent.do(task)
```

### From LangChain

```python
# LangChain
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(
    base_url="https://api.provider.com/v1",
    api_key="key"
)

# Upsonic
from upsonic import Agent
import os
os.environ["OPENAI_BASE_URL"] = "https://api.provider.com/v1"
os.environ["OPENAI_API_KEY"] = "key"
agent = Agent(model="openai/model")
```

---

## Next Steps

- [Native Model Providers](/native-providers) - OpenAI, Anthropic, Google
- [Cloud Model Providers](/cloud-providers) - AWS Bedrock, Azure OpenAI
- [Local Model Providers](/local-providers) - Ollama
- [Model Gateways](/model-gateways) - OpenRouter, LiteLLM
- [LLM Support Overview](/llm-overview) - Understanding LLM models

