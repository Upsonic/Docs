---
title: "OpenAI-Like Models"
description: "Using models that implement the OpenAI API specification"
---

## Overview

OpenAI-like models are LLM providers that implement the OpenAI API specification. You can use the standard `OpenAIChatModel` with custom `base_url` and `api_key` parameters to connect to any OpenAI-like endpoint.

**Model Class:** `OpenAIChatModel`

## Authentication

```python
from os import getenv
from upsonic.models.openai import OpenAIChatModel

model = OpenAIChatModel(
    model_name="your-model-name",
    api_key=getenv("YOUR_API_KEY"),
    base_url="https://your-custom-endpoint.com/v1"
)
```

## Examples

```python
from os import getenv
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel

model = OpenAIChatModel(
    model_name="your-model-name",
    api_key=getenv("YOUR_API_KEY"),
    base_url="https://your-custom-endpoint.com/v1"
)

agent = Agent(model=model)
task = Task("Hello, how are you?")
result = agent.do(task)
print(result)
```

## Parameters

| Parameter | Type | Description | Default | Source |
|-----------|------|-------------|---------|--------|
| `model_name` | `str` | Model identifier | Required | Base |
| `api_key` | `str` | API key for authentication | Required | Base |
| `base_url` | `str` | Custom endpoint URL | Required | Base |
| `max_tokens` | `int` | Maximum tokens to generate | Model default | Base |
| `temperature` | `float` | Sampling temperature (0.0-2.0) | 1.0 | Base |
| `top_p` | `float` | Nucleus sampling threshold | 1.0 | Base |
| `seed` | `int` | Random seed for reproducibility | None | Base |
| `stop_sequences` | `list[str]` | Sequences that stop generation | None | Base |
| `presence_penalty` | `float` | Penalize token presence (-2.0 to 2.0) | 0.0 | Base |
| `frequency_penalty` | `float` | Penalize token frequency (-2.0 to 2.0) | 0.0 | Base |
| `parallel_tool_calls` | `bool` | Allow parallel tool execution | True | Base |
| `timeout` | `float` | Request timeout in seconds | 600 | Base |
