---
title: "OpenAI"
description: "Using OpenAI models with Upsonic"
---

## Overview

OpenAI provides state-of-the-art language models including GPT-4o, o-series reasoning models, and GPT-5. Upsonic supports both the Chat Completions API and the newer Responses API.

**Model Classes:**
- `OpenAIChatModel`: For Chat Completions API
- `OpenAIResponsesModel`: For Responses API (recommended for new applications)

## Authentication

### Environment Variables

Set your OpenAI API key:

```bash
export OPENAI_API_KEY="sk-..."
export OPENAI_BASE_URL="https://api.openai.com/v1"  # Optional
```

### Direct Instantiation

```python
from upsonic.models.openai import OpenAIChatModel

# Automatic authentication from environment
model = OpenAIChatModel(model_name="gpt-4o")
```

### Manual Configuration

```python
from upsonic.models.openai import OpenAIChatModel, OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    max_tokens=2048,
    temperature=0.7
)

model = OpenAIChatModel(
    model_name="gpt-4o",
    settings=settings
)
```

### Custom Provider

```python
from upsonic.providers.openai import OpenAIProvider
from openai import AsyncOpenAI

# Custom client configuration
client = AsyncOpenAI(
    api_key="your-api-key",
    base_url="https://api.openai.com/v1",
    timeout=120.0
)

provider = OpenAIProvider(client=client)
model = OpenAIChatModel(model_name="gpt-4o", provider=provider)
```

## Examples

### Basic Usage

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel

model = OpenAIChatModel(model_name="gpt-4o")
agent = Agent(model=model)

task = Task("Explain quantum computing in simple terms")
result = agent.do(task)
print(result.output)
```

### With Reasoning Models

```python
from upsonic.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings

settings = OpenAIResponsesModelSettings(
    max_tokens=4096,
    openai_reasoning_effort="high",
    openai_reasoning_summary="detailed"
)

model = OpenAIResponsesModel(
    model_name="o1-preview",
    settings=settings
)

agent = Agent(model=model)
task = Task("Solve this complex math problem: ...")
result = agent.do(task)
```

### Streaming Responses

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel

model = OpenAIChatModel(model_name="gpt-4o")
agent = Agent(model=model)

task = Task("Write a long story about space exploration")

async with agent.stream(task) as result:
    async for text in result.stream_output():
        print(text, end='', flush=True)
```

### With Tools

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel

def get_weather(location: str) -> str:
    """Get the weather for a location."""
    return f"The weather in {location} is sunny."

model = OpenAIChatModel(model_name="gpt-4o")
agent = Agent(model=model)

task = Task(
    "What's the weather in San Francisco?",
    tools=[get_weather]
)
result = agent.do(task)
```

### Structured Output

```python
from pydantic import BaseModel
from upsonic.models.openai import OpenAIChatModel

class Analysis(BaseModel):
    sentiment: str
    confidence: float
    key_points: list[str]

model = OpenAIChatModel(model_name="gpt-4o")
model = model.with_structured_output(Analysis)

result = await model.ainvoke("Analyze this product review: ...")
print(result.sentiment)
print(result.confidence)
```

### With Vision

```python
from upsonic import Agent, Task
from upsonic.models.openai import OpenAIChatModel
from upsonic.messages import ImageUrl

model = OpenAIChatModel(model_name="gpt-4o")
agent = Agent(model=model)

task = Task(
    description="What's in this image?",
    attachments=[
        ImageUrl(url="https://example.com/image.jpg")
    ]
)

result = agent.do(task)
```

### Web Search (Search-Preview Models)

```python
from upsonic.models.openai import OpenAIChatModel, OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    max_tokens=2048
)

model = OpenAIChatModel(
    model_name="gpt-4o-search-preview-2025-03-11",
    settings=settings
)

agent = Agent(model=model)
task = Task("What are the latest AI developments in 2025?")
result = agent.do(task)
```

### Code Interpreter (o-series)

```python
from upsonic.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings

settings = OpenAIResponsesModelSettings(
    max_tokens=4096,
    openai_include_code_execution_outputs=True
)

model = OpenAIResponsesModel(
    model_name="o1-preview",
    settings=settings
)

agent = Agent(model=model)
task = Task("Create a plot showing the Fibonacci sequence up to 100")
result = agent.do(task)
```

## Prompt Caching

OpenAI supports automatic prompt caching to reduce latency and costs for repeated requests with similar contexts.

### How It Works

OpenAI automatically caches long prompts. When you send a request with a long context:
1. The first request processes normally and caches the prefix
2. Subsequent requests with the same prefix use the cache
3. Cached content is significantly cheaper and faster

### Best Practices

```python
from upsonic import Agent, Task

# Long system prompt that will be cached
system_prompt = """
You are an expert software engineer with deep knowledge of:
- Python, Java, JavaScript, and Go
- Database design and optimization
- Cloud architecture (AWS, Azure, GCP)
- Security best practices
... (more context)
"""

from upsonic.models.openai import OpenAIChatModel

model = OpenAIChatModel(model_name="gpt-4o")
agent = Agent(
    model=model,
    system_prompt=system_prompt
)

# First request - builds cache
task1 = Task("Review this Python code for security issues")
result1 = agent.do(task1)

# Subsequent requests - uses cache (faster and cheaper)
task2 = Task("Optimize this database query")
result2 = agent.do(task2)
```

### Cache Efficiency

- **Minimum Length**: Prompts should be at least 1024 tokens to benefit
- **Cost**: Cached tokens cost 50% less than regular input tokens
- **Latency**: Cached requests are 2-4x faster
- **TTL**: Cache expires after 5-10 minutes of inactivity

### With Memory

```python
from upsonic import Agent, Task
from upsonic.storage.memory import Memory
from upsonic.storage.providers.in_memory import InMemoryStorage

# Memory automatically benefits from caching
storage = InMemoryStorage()
memory = Memory(storage=storage, session_id="user-123")

from upsonic.models.openai import OpenAIChatModel

model = OpenAIChatModel(model_name="gpt-4o")
agent = Agent(model=model, memory=memory)

# Conversation history is cached
task1 = Task("Tell me about quantum physics")
result1 = agent.do(task1)

task2 = Task("Can you explain that in simpler terms?")
result2 = agent.do(task2)  # Previous context is cached
```

## Model Parameters

### Base Model Settings

| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `max_tokens` | `int` | Maximum tokens to generate | Model-specific |
| `temperature` | `float` | Sampling temperature (0.0-2.0)* | 1.0 |
| `top_p` | `float` | Nucleus sampling threshold* | 1.0 |
| `seed` | `int` | Random seed for deterministic outputs | None |
| `stop_sequences` | `list[str]` | Sequences that stop generation | None |
| `presence_penalty` | `float` | Penalty for token presence (-2.0 to 2.0)* | 0.0 |
| `frequency_penalty` | `float` | Penalty for token frequency (-2.0 to 2.0)* | 0.0 |
| `logit_bias` | `dict[str, int]` | Modify token likelihoods | None |
| `parallel_tool_calls` | `bool` | Allow parallel tool calls | True |
| `timeout` | `float` | Request timeout in seconds | 600 |
| `extra_headers` | `dict[str, str]` | Additional HTTP headers | None |
| `extra_body` | `object` | Additional request body params | None |

**Note:** * Not available on reasoning models (o1, o3, GPT-5)

### OpenAI-Specific Settings (Chat Completions)

| Parameter | Type | Description |
|-----------|------|-------------|
| `openai_reasoning_effort` | `'low' \| 'medium' \| 'high'` | Computational effort for reasoning models |
| `openai_logprobs` | `bool` | Include log probabilities in response |
| `openai_top_logprobs` | `int` | Number of top log probs to return (1-20) |
| `openai_user` | `str` | Unique user identifier for abuse monitoring |
| `openai_service_tier` | `'auto' \| 'default' \| 'flex' \| 'priority'` | Service tier for request routing |
| `openai_prediction` | `ChatCompletionPredictionContentParam` | Enable predicted outputs |

### OpenAI-Specific Settings (Responses API)

| Parameter | Type | Description |
|-----------|------|-------------|
| `openai_reasoning_summary` | `'detailed' \| 'concise'` | Reasoning summary detail level |
| `openai_send_reasoning_ids` | `bool` | Send reasoning part IDs in history |
| `openai_truncation` | `'disabled' \| 'auto'` | Context window truncation strategy |
| `openai_text_verbosity` | `'low' \| 'medium' \| 'high'` | Text response verbosity |
| `openai_previous_response_id` | `'auto' \| str` | Continue from previous response |
| `openai_include_code_execution_outputs` | `bool` | Include code interpreter outputs |
| `openai_include_web_search_sources` | `bool` | Include web search sources |
| `openai_builtin_tools` | `Sequence[...]` | Built-in tools to enable |

### Example with All Settings

```python
from upsonic.models.openai import OpenAIChatModel, OpenAIChatModelSettings

settings = OpenAIChatModelSettings(
    # Base settings
    max_tokens=2048,
    temperature=0.7,
    top_p=0.9,
    seed=42,
    stop_sequences=["END"],
    presence_penalty=0.1,
    frequency_penalty=0.1,
    parallel_tool_calls=True,
    timeout=120.0,
    
    # OpenAI-specific
    openai_reasoning_effort="high",
    openai_logprobs=True,
    openai_top_logprobs=5,
    openai_user="user-12345",
    openai_service_tier="priority",
    
    # Custom headers
    extra_headers={"X-Custom-Header": "value"}
)

model = OpenAIChatModel(
    model_name="gpt-4o",
    settings=settings
)
```

## Available Models

### GPT-4o Models
- `gpt-4o`: Most capable multimodal model
- `gpt-4o-mini`: Fast and affordable
- `gpt-4o-audio-preview`: With audio capabilities
- `gpt-4o-search-preview`: With web search

### Reasoning Models (o-series)
- `o1-preview`: Best reasoning capabilities
- `o1-mini`: Fast reasoning model
- `o3`: Next-generation reasoning
- `o3-mini`: Efficient reasoning

### GPT-5 Models
- `gpt-5`: Next-generation flagship model
- `gpt-5-mini`: Efficient GPT-5 variant

### Legacy Models
- `gpt-4`: Original GPT-4
- `gpt-4-turbo`: Faster GPT-4 variant
- `gpt-3.5-turbo`: Most affordable option

## Best Practices

1. **Use Responses API for New Applications**: Better support for reasoning and built-in tools
2. **Enable Prompt Caching**: Structure prompts to maximize cache hits
3. **Set Reasonable Timeouts**: Reasoning models can take longer
4. **Use Streaming**: Improves user experience for long responses
5. **Implement Retry Logic**: Handle rate limits and transient errors
6. **Monitor Usage**: Track token usage to manage costs
7. **Use Structured Output**: Get type-safe responses with Pydantic

## Related Resources

- [OpenAI API Documentation](https://platform.openai.com/docs)
- [OpenAI Models Overview](https://platform.openai.com/docs/models)
- [OpenAI Pricing](https://openai.com/api/pricing/)

