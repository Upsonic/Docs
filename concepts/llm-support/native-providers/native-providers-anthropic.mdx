---
title: "Anthropic"
description: "Using Anthropic Claude models with Upsonic"
---

## Overview

Anthropic provides the Claude family of models known for safety, accuracy, and extended thinking capabilities. Upsonic supports all Claude models including Claude 3.5, Claude 3.7, and Claude 4.

**Model Class:** `AnthropicModel`

## Authentication

### Environment Variables

Set your Anthropic API key:

```bash
export ANTHROPIC_API_KEY="sk-ant-..."
```

### Using infer_model

```python
from upsonic import infer_model

# Automatic authentication from environment
model = infer_model("anthropic/claude-3-5-sonnet-20241022")
```

### Manual Configuration

```python
from upsonic.models.anthropic import AnthropicModel, AnthropicModelSettings

settings = AnthropicModelSettings(
    max_tokens=4096,
    temperature=0.7
)

model = AnthropicModel(
    model_name="claude-3-5-sonnet-20241022",
    settings=settings
)
```

### Custom Provider

```python
from upsonic.providers.anthropic import AnthropicProvider
from anthropic import AsyncAnthropic

# Custom client configuration
client = AsyncAnthropic(
    api_key="your-api-key",
    timeout=120.0
)

provider = AnthropicProvider(client=client)
model = AnthropicModel(
    model_name="claude-3-5-sonnet-20241022",
    provider=provider
)
```

## Examples

### Basic Usage

```python
from upsonic import Agent, Task, infer_model

model = infer_model("anthropic/claude-3-5-sonnet-20241022")
agent = Agent(model=model)

task = Task("Explain the theory of relativity")
result = agent.do(task)
print(result.output)
```

### With Extended Thinking

```python
from upsonic.models.anthropic import AnthropicModel, AnthropicModelSettings

settings = AnthropicModelSettings(
    max_tokens=8192,
    temperature=0.1,
    anthropic_thinking={
        "type": "enabled",
        "budget_tokens": 10000
    }
)

model = AnthropicModel(
    model_name="claude-3-5-sonnet-20241022",
    settings=settings
)

agent = Agent(
    model=model,
    enable_thinking_tool=True
)

task = Task("Solve this complex logic puzzle step by step: ...")
result = agent.do(task)
```

### Streaming Responses

```python
from upsonic import Agent, Task, infer_model

model = infer_model("anthropic/claude-3-5-sonnet-20241022")
agent = Agent(model=model)

task = Task("Write a detailed analysis of climate change")

async for chunk in agent.do_stream(task):
    print(chunk, end="", flush=True)
```

### With Tools

```python
from upsonic import Agent, Task, infer_model

def search_database(query: str) -> str:
    """Search the database for information."""
    return f"Results for: {query}"

def calculate(expression: str) -> float:
    """Evaluate a mathematical expression."""
    return eval(expression)

model = infer_model("anthropic/claude-3-5-sonnet-20241022")
agent = Agent(
    model=model,
    tools=[search_database, calculate]
)

task = Task("Find information about quantum computing and calculate 25 * 36")
result = agent.do(task)
```

### With Vision

```python
from upsonic import Agent, Task, infer_model
from upsonic.messages import ImageUrl

model = infer_model("anthropic/claude-3-5-sonnet-20241022")
agent = Agent(model=model)

task = Task(
    description="Describe this image in detail",
    attachments=[
        ImageUrl(url="https://example.com/image.jpg")
    ]
)

result = agent.do(task)
```

### Web Search (Built-in Tool)

```python
from upsonic import Agent, Task, infer_model
from upsonic.tools.builtin_tools import WebSearchTool

model = infer_model("anthropic/claude-3-7-sonnet-20250219")
agent = Agent(
    model=model,
    builtin_tools=[WebSearchTool()]
)

task = Task("What are the latest developments in AI in 2025?")
result = agent.do(task)
```

### Code Execution (Built-in Tool)

```python
from upsonic import Agent, Task, infer_model
from upsonic.tools.builtin_tools import CodeExecutionTool

model = infer_model("anthropic/claude-3-7-sonnet-20250219")
agent = Agent(
    model=model,
    builtin_tools=[CodeExecutionTool()]
)

task = Task("Create a visualization of the Fibonacci sequence")
result = agent.do(task)
```

### With Metadata

```python
from upsonic.models.anthropic import AnthropicModel, AnthropicModelSettings

settings = AnthropicModelSettings(
    max_tokens=2048,
    anthropic_metadata={
        "user_id": "user-12345"
    }
)

model = AnthropicModel(
    model_name="claude-3-5-sonnet-20241022",
    settings=settings
)

agent = Agent(model=model)
task = Task("Analyze this customer feedback")
result = agent.do(task)
```

## Prompt Caching

Anthropic supports prompt caching to reduce latency and costs for repeated requests with similar contexts.

### How It Works

Claude automatically caches long contexts. When using prompt caching:
1. The first request processes the full prompt and caches specified parts
2. Subsequent requests with the same cached content are faster and cheaper
3. Caches persist for 5 minutes and refresh on each use

### Best Practices

```python
from upsonic import Agent, Task, infer_model

# Long system prompt that will be cached
system_prompt = """
You are an expert code reviewer with knowledge of:
- Security best practices (OWASP Top 10)
- Performance optimization techniques
- Code quality standards (SOLID, DRY, KISS)
- Testing strategies
... (extensive guidelines - 1000+ tokens)
"""

model = infer_model("anthropic/claude-3-5-sonnet-20241022")
agent = Agent(
    model=model,
    system_prompt=system_prompt
)

# First request - builds cache
task1 = Task("Review this authentication code")
result1 = agent.do(task1)

# Subsequent requests - uses cache (4x faster, 90% cheaper for cached portion)
task2 = Task("Review this API endpoint")
result2 = agent.do(task2)
```

### Cache Efficiency

- **Minimum Length**: Cached sections should be at least 1024 tokens
- **Cost Savings**: Cached input costs 90% less than regular input
- **Speed Improvement**: 4-10x faster for cached content
- **TTL**: Cache expires after 5 minutes of inactivity
- **Auto-Refresh**: Each cache hit extends the TTL

### Structured Caching

```python
from upsonic import Agent, Task, infer_model

# Structure prompts to maximize caching
documentation = """... extensive API documentation ..."""
guidelines = """... coding guidelines ..."""
examples = """... code examples ..."""

# These will be cached as a block
system_prompt = f"{documentation}\n\n{guidelines}\n\n{examples}"

model = infer_model("anthropic/claude-3-5-sonnet-20241022")
agent = Agent(model=model, system_prompt=system_prompt)

# All requests benefit from cached documentation
for code_file in code_files:
    task = Task(f"Review this code:\n\n{code_file}")
    result = agent.do(task)
```

### With Memory

```python
from upsonic import Agent, Task, infer_model
from upsonic.storage.memory import Memory
from upsonic.storage.providers.in_memory import InMemoryStorage

# Conversation history is automatically cached
storage = InMemoryStorage()
memory = Memory(storage=storage, session_id="session-123")

model = infer_model("anthropic/claude-3-5-sonnet-20241022")
agent = Agent(model=model, memory=memory)

# Long conversation - history is cached
task1 = Task("Let's discuss quantum mechanics")
result1 = agent.do(task1)

task2 = Task("Can you explain wave-particle duality?")
result2 = agent.do(task2)  # Previous context cached
```

## Model Parameters

### Base Model Settings

| Parameter | Type | Description | Default |
|-----------|------|-------------|---------|
| `max_tokens` | `int` | Maximum tokens to generate | 4096 |
| `temperature` | `float` | Sampling temperature (0.0-1.0) | 1.0 |
| `top_p` | `float` | Nucleus sampling threshold | 1.0 |
| `top_k` | `int` | Top-k sampling parameter | None |
| `stop_sequences` | `list[str]` | Sequences that stop generation | None |
| `timeout` | `float` | Request timeout in seconds | 600 |
| `extra_headers` | `dict[str, str]` | Additional HTTP headers | None |

### Anthropic-Specific Settings

| Parameter | Type | Description |
|-----------|------|-------------|
| `anthropic_metadata` | `dict` | Request metadata with user_id |
| `anthropic_thinking` | `dict` | Extended thinking configuration |

#### Thinking Configuration

```python
anthropic_thinking = {
    "type": "enabled",        # Enable extended thinking
    "budget_tokens": 10000    # Maximum tokens for thinking
}
```

Options:
- `type`: `"enabled"` or `"disabled"`
- `budget_tokens`: Maximum tokens allocated to thinking (1000-100000)

### Example with All Settings

```python
from upsonic.models.anthropic import AnthropicModel, AnthropicModelSettings

settings = AnthropicModelSettings(
    # Base settings
    max_tokens=8192,
    temperature=0.7,
    top_p=0.9,
    top_k=50,
    stop_sequences=["END", "STOP"],
    timeout=120.0,
    
    # Anthropic-specific
    anthropic_metadata={
        "user_id": "user-12345"
    },
    anthropic_thinking={
        "type": "enabled",
        "budget_tokens": 10000
    },
    
    # Custom headers
    extra_headers={"X-Custom-Header": "value"}
)

model = AnthropicModel(
    model_name="claude-3-5-sonnet-20241022",
    settings=settings
)
```

## Available Models

### Claude 3.5 Series
- `claude-3-5-sonnet-20241022`: Most capable and balanced
- `claude-3-5-sonnet-latest`: Always points to latest sonnet
- `claude-3-5-haiku-20241022`: Fast and affordable
- `claude-3-5-haiku-latest`: Always points to latest haiku

### Claude 3.7 Series
- `claude-3-7-sonnet-20250219`: Enhanced capabilities
- `claude-3-7-sonnet-latest`: Always points to latest

### Claude 4 Series
- `claude-4-opus-20250514`: Most powerful model
- `claude-4-sonnet-20250514`: Balanced performance
- `claude-sonnet-4-0`: Alias for latest sonnet
- `claude-opus-4-0`: Alias for latest opus

### Claude 3 Series (Legacy)
- `claude-3-opus-20240229`: Original most capable
- `claude-3-sonnet-20240229`: Balanced option
- `claude-3-haiku-20240307`: Fastest option

## Best Practices

1. **Use Extended Thinking for Complex Tasks**: Enable thinking for reasoning, math, and logic problems
2. **Leverage Prompt Caching**: Structure prompts to maximize cache hits
3. **Enable Built-in Tools**: Use web search and code execution when needed
4. **Set Appropriate Token Budgets**: Allocate sufficient thinking budget for complex tasks
5. **Use Latest Models**: `latest` aliases always point to best models
6. **Monitor Caching**: Check response headers for cache hit information
7. **Structure System Prompts**: Place stable content first for better caching

## Safety and Content Policy

Claude models have built-in safety features:

- **Constitutional AI**: Models are trained to be helpful, harmless, and honest
- **Automatic Moderation**: Refuses harmful or inappropriate requests
- **No Additional Configuration**: Safety is built-in and always active

## Related Resources

- [Anthropic API Documentation](https://docs.anthropic.com/)
- [Claude Models Overview](https://www.anthropic.com/claude)
- [Anthropic Pricing](https://www.anthropic.com/pricing)
- [Prompt Caching Guide](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)
- [Extended Thinking Guide](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)

