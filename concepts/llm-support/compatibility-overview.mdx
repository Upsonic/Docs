---
title: "LLM Compatibility Overview"
description: "Comprehensive feature support and comparison tables across model providers"
---

## Feature Support Matrix

Complete feature availability across all providers:

| Provider | Tools | JSON Schema | JSON Object | Streaming | Vision | Audio | Web Search | Code Execution | Thinking/Reasoning | Prompt Caching |
|----------|-------|-------------|-------------|-----------|--------|-------|------------|----------------|-------------------|----------------|
| **Native Providers** |
| OpenAI | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅* | ✅* | ✅** | ✅ |
| Anthropic | ✅ | ✅ | ❌ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ |
| Google (Gemini) | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| Mistral | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ❌ |
| Cohere | ✅ | ❌ | ❌ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ |
| Grok | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ |
| **Cloud Providers** |
| Azure OpenAI | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅* | ✅* | ✅** | ✅ |
| AWS Bedrock | ✅ | ✅ | ❌ | ✅ | ✅ | ❌ | ❌ | ❌ | ✅*** | ✅ |
| Hugging Face | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ |
| **Local Providers** |
| Ollama | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ |
| **Model Gateways** |
| OpenRouter | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | Varies | Varies | Varies | ❌ |
| LiteLLM | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | Varies | Varies | Varies | ❌ |
| Groq | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ |
| **OpenAI-Compatible** |
| DeepSeek | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ |
| Cerebras | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ | ❌ |
| Fireworks | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ❌ |
| GitHub Models | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | Varies | ❌ |
| Together AI | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ❌ | Varies | ❌ |

**Legend:**
- `*` Only on specific model variants
- `**` Only on reasoning models (o1, o3, GPT-5)
- `***` Depends on underlying Bedrock model
- `Varies` Depends on model accessed through gateway

## Model Settings Support

Common parameters and their availability:

| Setting | OpenAI | Anthropic | Google | Mistral | Cohere | Groq | Bedrock | Ollama | HuggingFace |
|---------|--------|-----------|--------|---------|--------|------|---------|--------|-------------|
| `max_tokens` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| `temperature` | ✅* | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| `top_p` | ✅* | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| `top_k` | ❌ | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ |
| `seed` | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ |
| `stop_sequences` | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| `presence_penalty` | ✅* | ❌ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ |
| `frequency_penalty` | ✅* | ❌ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ | ✅ |
| `logit_bias` | ✅ | ❌ | ❌ | ❌ | ❌ | ✅ | ❌ | ❌ | ✅ |
| `parallel_tool_calls` | ✅ | ✅ | ❌ | ❌ | ❌ | ✅ | ❌ | ❌ | ❌ |
| `timeout` | ✅ | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ | ❌ | ❌ |

**Note:** `*` Not available on reasoning models

## Context Window Sizes

Maximum context lengths by provider and model:

| Provider | Model | Max Context | Max Output | Notes |
|----------|-------|-------------|------------|-------|
| **OpenAI** |
| OpenAI | gpt-4o | 128K | 16K | Standard |
| OpenAI | gpt-4o-mini | 128K | 16K | Standard |
| OpenAI | o1-preview | 128K | 32K | Reasoning |
| OpenAI | o3-mini | 200K | 100K | Extended |
| OpenAI | gpt-5 | 256K | 64K | Latest |
| **Anthropic** |
| Anthropic | claude-3-5-sonnet | 200K | 8K | Cached context supported |
| Anthropic | claude-3-5-haiku | 200K | 8K | Fast variant |
| Anthropic | claude-opus-4 | 200K | 8K | Most capable |
| **Google** |
| Google | gemini-2.5-pro | 2M | 8K | Largest context |
| Google | gemini-2.5-flash | 1M | 8K | Balanced |
| Google | gemini-2.5-flash-lite | 1M | 8K | Efficient |
| **Mistral** |
| Mistral | mistral-large | 128K | 8K | Latest |
| Mistral | mistral-small | 32K | 8K | Fast |
| **Others** |
| Cohere | command-r-plus | 128K | 4K | Standard |
| Groq | llama-3.3-70b | 128K | 8K | Fast inference |
| Grok | grok-4 | 128K | 4K | Standard |
| Bedrock | claude-3.5-sonnet | 200K | 8K | Via Bedrock |
| Ollama | llama3.2 | 128K | 8K | Local |

## Authentication Methods

Required credentials by provider:

| Provider | Auth Method | Environment Variable | Additional Setup |
|----------|-------------|---------------------|------------------|
| OpenAI | API Key | `OPENAI_API_KEY` | None |
| Anthropic | API Key | `ANTHROPIC_API_KEY` | None |
| Google (GLA) | API Key | `GOOGLE_API_KEY` or `GEMINI_API_KEY` | None |
| Google (Vertex) | GCP Auth | `GOOGLE_CLOUD_PROJECT` | `gcloud auth` |
| Mistral | API Key | `MISTRAL_API_KEY` | None |
| Cohere | API Key | `CO_API_KEY` | None |
| Groq | API Key | `GROQ_API_KEY` | None |
| Grok | API Key | `GROK_API_KEY` | None |
| Azure OpenAI | API Key + Endpoint | `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT` | Resource setup |
| AWS Bedrock | AWS Credentials | `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` | IAM permissions |
| Hugging Face | Token | `HF_TOKEN` | None |
| Ollama | None (Local) | `OLLAMA_BASE_URL` (optional) | Install Ollama |
| OpenRouter | API Key | `OPENROUTER_API_KEY` | None |
| LiteLLM | Proxy Setup | Various (via config) | Run proxy server |
| DeepSeek | API Key | `DEEPSEEK_API_KEY` | None |
| Cerebras | API Key | `CEREBRAS_API_KEY` | None |
| Fireworks | API Key | `FIREWORKS_API_KEY` | None |

## Model Classes Reference

Upsonic model class to use for each provider:

| Provider | Model Class | Base Class | Import Path |
|----------|-------------|------------|-------------|
| OpenAI (Chat) | `OpenAIChatModel` | `Model` | `upsonic.models.openai` |
| OpenAI (Responses) | `OpenAIResponsesModel` | `Model` | `upsonic.models.openai` |
| Anthropic | `AnthropicModel` | `Model` | `upsonic.models.anthropic` |
| Google | `GoogleModel` | `Model` | `upsonic.models.google` |
| Mistral | `MistralModel` | `Model` | `upsonic.models.mistral` |
| Cohere | `CohereModel` | `Model` | `upsonic.models.cohere` |
| Groq | `GroqModel` | `Model` | `upsonic.models.groq` |
| AWS Bedrock | `BedrockConverseModel` | `Model` | `upsonic.models.bedrock` |
| Hugging Face | `HuggingFaceModel` | `Model` | `upsonic.models.huggingface` |
| Grok | `OpenAIChatModel` | `Model` | `upsonic.models.openai` |
| Azure | `OpenAIChatModel` | `Model` | `upsonic.models.openai` |
| Ollama | `OpenAIChatModel` | `Model` | `upsonic.models.openai` |
| DeepSeek | `OpenAIChatModel` | `Model` | `upsonic.models.openai` |
| Cerebras | `OpenAIChatModel` | `Model` | `upsonic.models.openai` |
| Fireworks | `OpenAIChatModel` | `Model` | `upsonic.models.openai` |
| GitHub | `OpenAIChatModel` | `Model` | `upsonic.models.openai` |
| Together | `OpenAIChatModel` | `Model` | `upsonic.models.openai` |

## Provider-Specific Settings

Unique configuration parameters by provider:

| Provider | Parameter Prefix | Key Settings | Description |
|----------|------------------|--------------|-------------|
| OpenAI | `openai_` | `reasoning_effort`, `service_tier`, `prediction` | Reasoning control, priority access, predicted outputs |
| Anthropic | `anthropic_` | `thinking`, `metadata` | Extended thinking config, request metadata |
| Google | `google_` | `thinking_config`, `safety_settings`, `cached_content`, `labels` | Thinking budget, content safety, caching, billing labels |
| Groq | `groq_` | `reasoning_format` | Reasoning output format (hidden/raw/parsed) |
| Bedrock | `bedrock_` | `guardrail_config`, `performance_configuration` | Content moderation, latency optimization |
| None | N/A | None | Mistral, Cohere, Ollama, HuggingFace use base settings only |

## Prompt Caching Comparison

Caching capabilities and efficiency:

| Provider | Support | Min Tokens | Cost Reduction | Speed Improvement | TTL | Notes |
|----------|---------|------------|----------------|-------------------|-----|-------|
| OpenAI | ✅ | 1024 | 50% | 2-4x | 5-10 min | Automatic |
| Anthropic | ✅ | 1024 | 90% | 4-10x | 5 min | Refreshes on use |
| Google | ✅ | 32768 | 75% | 3-5x | 60 min | Context caching |
| Azure OpenAI | ✅ | 1024 | 50% | 2-4x | 5-10 min | Same as OpenAI |
| Bedrock (Claude) | ✅ | 1024 | 90% | 4-10x | 5 min | Via Claude models |
| Mistral | ❌ | N/A | N/A | N/A | N/A | Not supported |
| Cohere | ❌ | N/A | N/A | N/A | N/A | Not supported |
| Groq | ❌ | N/A | N/A | N/A | N/A | Not supported |
| Grok | ❌ | N/A | N/A | N/A | N/A | Not supported |
| Others | ❌ | N/A | N/A | N/A | N/A | Not supported |

## Performance Comparison

Approximate inference speeds (tokens/second):

| Provider | Model Example | Speed (tok/s) | Latency (TTFT) | Best For |
|----------|---------------|---------------|----------------|----------|
| Groq | llama-3.3-70b | ~700 | <100ms | Speed-critical apps |
| Cerebras | llama-3.3-70b | ~500 | <150ms | High throughput |
| Fireworks | llama-3.1-70b | ~200 | ~200ms | Balanced |
| OpenAI | gpt-4o | ~150 | ~300ms | Quality priority |
| Anthropic | claude-3.5-sonnet | ~120 | ~400ms | Reasoning tasks |
| Google | gemini-2.5-flash | ~180 | ~250ms | Multimodal |
| Mistral | mistral-large | ~150 | ~300ms | Standard |
| Ollama | llama3.2 (local) | 50-200 | ~500ms | Local, hardware dependent |
| Bedrock | Varies by model | 100-200 | ~500ms | Enterprise integration |

**Note:** Performance varies by model size, load, and network conditions

## Pricing Comparison (Input/Output per 1M tokens)

Cost efficiency across providers:

| Provider | Model | Input Price | Output Price | Total (1M in + 1M out) |
|----------|-------|-------------|--------------|------------------------|
| **Native Providers** |
| OpenAI | gpt-4o | $2.50 | $10.00 | $12.50 |
| OpenAI | gpt-4o-mini | $0.15 | $0.60 | $0.75 |
| Anthropic | claude-3.5-sonnet | $3.00 | $15.00 | $18.00 |
| Anthropic | claude-3.5-haiku | $0.80 | $4.00 | $4.80 |
| Google | gemini-2.5-pro | $1.25 | $5.00 | $6.25 |
| Google | gemini-2.5-flash | $0.075 | $0.30 | $0.375 |
| Mistral | mistral-large | $2.00 | $6.00 | $8.00 |
| Cohere | command-r-plus | $2.50 | $10.00 | $12.50 |
| **Cost-Effective Options** |
| DeepSeek | deepseek-chat | $0.14 | $0.28 | $0.42 |
| Groq | llama-3.3-70b | $0.59 | $0.79 | $1.38 |
| Cerebras | llama-3.3-70b | $0.60 | $0.60 | $1.20 |
| Ollama | Any (local) | $0.00 | $0.00 | $0.00 |
| **Cloud Providers** |
| Azure | Same as OpenAI | Same | Same | Same as OpenAI |
| Bedrock | Varies by model | Varies | Varies | Model-dependent |

**Note:** Prices are approximate and subject to change. Check provider pricing pages for current rates.

## Rate Limits Comparison

Request limits by provider tier:

| Provider | Free Tier | Paid Tier | Enterprise | Notes |
|----------|-----------|-----------|------------|-------|
| OpenAI | N/A | 3,500 RPM* | Custom | *Varies by model and tier |
| Anthropic | N/A | 4,000 RPM | Custom | Higher for cached requests |
| Google (GLA) | 15 RPM | 360 RPM | Custom | Free tier available |
| Google (Vertex) | N/A | Quota-based | Custom | GCP quotas apply |
| Mistral | Limited | 500 RPM | Custom | Variable by model |
| Cohere | 100 calls/min | Higher | Custom | Trial available |
| Groq | 30 RPM | 14,400 RPM | Custom | Very generous paid tier |
| Grok | Limited | Higher | Custom | Variable |
| Ollama | Unlimited | N/A | N/A | Local only |
| OpenRouter | Pay-as-go | Pay-as-go | Pay-as-go | No fixed limits |
| DeepSeek | 60 RPM | Higher | Custom | Cost-effective |

**RPM = Requests Per Minute**

## Enterprise Features

Business and compliance capabilities:

| Feature | OpenAI | Anthropic | Google | Azure | Bedrock | Others |
|---------|--------|-----------|--------|-------|---------|--------|
| SSO/SAML | Enterprise | Enterprise | ✅ | ✅ | ✅ | Varies |
| SOC 2 | ✅ | ✅ | ✅ | ✅ | ✅ | Varies |
| HIPAA | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ |
| Data Residency | Limited | Limited | ✅ | ✅ | ✅ | ❌ |
| Private Endpoints | ❌ | ❌ | ✅ | ✅ | ✅ | ❌ |
| Custom SLA | Enterprise | Enterprise | ✅ | ✅ | ✅ | Varies |
| Audit Logs | ✅ | ✅ | ✅ | ✅ | ✅ | Varies |
| Content Filtering | ✅ | ✅ | ✅ | ✅ | ✅ | Varies |
| Usage Analytics | ✅ | ✅ | ✅ | ✅ | ✅ | Varies |

## Provider Selection Matrix

Quick reference for choosing providers:

| Use Case | Primary Choice | Alternative | Budget Option |
|----------|----------------|-------------|---------------|
| Production (General) | OpenAI | Anthropic | DeepSeek |
| Reasoning Tasks | OpenAI (o-series) | DeepSeek R1 | Anthropic |
| Vision Tasks | OpenAI (GPT-4o) | Google Gemini | Anthropic Claude |
| Code Generation | OpenAI | Anthropic | DeepSeek |
| Multimodal | Google Gemini | OpenAI | Anthropic |
| Speed Critical | Groq | Cerebras | Fireworks |
| Cost Optimization | DeepSeek | Groq | Google Flash |
| Enterprise/Compliance | Azure OpenAI | AWS Bedrock | Google Vertex |
| Local Development | Ollama | N/A | N/A |
| High Volume | Groq | Cerebras | DeepSeek |
| Multilingual | Cohere | Google Gemini | Mistral |
| Real-time Info | Grok | Groq | Anthropic |

## Next Steps

- Explore [Native Providers](/native-providers/openai) for direct API access
- Check [Local Providers](/local-providers/ollama) for local development
- Learn about [Model Gateways](/model-gateways/openrouter) for unified access
- Review [OpenAI-Compatible](/openai-compatible/overview) providers for flexibility

